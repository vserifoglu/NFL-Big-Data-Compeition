{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24184e40-0fd3-41a1-bede-132f800b1cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandera in /home/veysel/dev-projects/NFL-Big-Data-Compeition/venv/lib/python3.12/site-packages (0.27.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/veysel/dev-projects/NFL-Big-Data-Compeition/venv/lib/python3.12/site-packages (from pandera) (25.0)\n",
      "Requirement already satisfied: pydantic in /home/veysel/dev-projects/NFL-Big-Data-Compeition/venv/lib/python3.12/site-packages (from pandera) (2.12.5)\n",
      "Requirement already satisfied: typeguard in /home/veysel/dev-projects/NFL-Big-Data-Compeition/venv/lib/python3.12/site-packages (from pandera) (4.4.4)\n",
      "Requirement already satisfied: typing_extensions in /home/veysel/dev-projects/NFL-Big-Data-Compeition/venv/lib/python3.12/site-packages (from pandera) (4.15.0)\n",
      "Requirement already satisfied: typing_inspect>=0.6.0 in /home/veysel/dev-projects/NFL-Big-Data-Compeition/venv/lib/python3.12/site-packages (from pandera) (0.9.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/veysel/dev-projects/NFL-Big-Data-Compeition/venv/lib/python3.12/site-packages (from typing_inspect>=0.6.0->pandera) (1.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/veysel/dev-projects/NFL-Big-Data-Compeition/venv/lib/python3.12/site-packages (from pydantic->pandera) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/veysel/dev-projects/NFL-Big-Data-Compeition/venv/lib/python3.12/site-packages (from pydantic->pandera) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/veysel/dev-projects/NFL-Big-Data-Compeition/venv/lib/python3.12/site-packages (from pydantic->pandera) (0.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "942aec17-5fd8-42c9-b098-24d48bda297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "from matplotlib.patches import Circle, Ellipse\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import gc\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from datetime import datetime\n",
    "import pandera.pandas as pa\n",
    "from pandera.typing import Series\n",
    "from IPython.display import HTML\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from typing import Generator, Tuple, List\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "from matplotlib.patches import Circle, Ellipse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5470592f-602e-49f2-b488-7227a9c396b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPipelineConfig(BaseModel):\n",
    "    DATA_DIR: str = \"../data/train\"\n",
    "    SUPP_FILE: str = \"../data/supplementary_data.csv\"\n",
    "    OUTPUT_DIR: str = \"../data/processed\"\n",
    "\n",
    "\n",
    "class VisPipelineConfig(BaseModel):\n",
    "    OUTPUT_DIR: str = \"../static/visuals_test\"\n",
    "    TRACKING_FILE: str = \"../data/processed/master_animation_data.csv\"\n",
    "    SUMMARY_FILE: str = \"../data/processed/eraser_analysis_summary.csv\"\n",
    "\n",
    "\n",
    "# Default config instance\n",
    "data_config = DataPipelineConfig()\n",
    "vis_config = VisPipelineConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04cae8b3-2f64-4703-a0f9-a0e4a223102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.read_csv(vis_config.SUMMARY_FILE)\n",
    "tracking_df = pd.read_csv(vis_config.TRACKING_FILE)\n",
    "\n",
    "supp_df = pd.read_csv(vis_config.TRACKING_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda3028f-e4ec-4db6-a826-2c1126c5fe9d",
   "metadata": {},
   "source": [
    "## Data Processing Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07095869-7ceb-4e1e-89de-655a1395af44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawSuppSchema(pa.DataFrameModel):\n",
    "    \"\"\"\n",
    "    Validates 'supplementary_data.csv'.\n",
    "    Contains Game Context, Play Types\n",
    "    \"\"\"\n",
    "\n",
    "    game_id: Series[int] = pa.Field(coerce=True)\n",
    "    play_id: Series[int] = pa.Field(coerce=True)\n",
    "    \n",
    "\n",
    "    week: Series[int] = pa.Field(coerce=True)\n",
    "    home_team_abbr: Series[str]\n",
    "    visitor_team_abbr: Series[str]\n",
    "    \n",
    "\n",
    "    down: Series[int] = pa.Field(coerce=True, ge=1, le=4)\n",
    "    yards_to_go: Series[int] = pa.Field(coerce=True)\n",
    "    possession_team: Series[str]\n",
    "    yardline_side: Series[str] = pa.Field(nullable=True)\n",
    "    yardline_number: Series[int] = pa.Field(ge=0, le=50) \n",
    "    defensive_team: Series[str] = pa.Field(nullable=True)\n",
    "    \n",
    "\n",
    "    pre_snap_home_team_win_probability: Series[float] = pa.Field(nullable=True)\n",
    "    pre_snap_visitor_team_win_probability: Series[float] = pa.Field(nullable=True)\n",
    "\n",
    "\n",
    "    play_nullified_by_penalty: Series[str] = pa.Field(nullable=True)\n",
    "    dropback_type: Series[str] = pa.Field(nullable=True) \n",
    "    team_coverage_man_zone: Series[str] = pa.Field(nullable=True)\n",
    "    team_coverage_type: Series[str] = pa.Field(nullable=True) \n",
    "    pass_result: Series[str] = pa.Field(nullable=True) \n",
    "    pass_length: Series[int] = pa.Field(nullable=True)\n",
    "    route_of_targeted_receiver: Series[str] = pa.Field(nullable=True)\n",
    "    yards_gained: Series[int] = pa.Field(coerce=True, nullable=True)\n",
    "    expected_points_added: Series[float] = pa.Field(coerce=True, nullable=True)\n",
    "\n",
    "    class Config:\n",
    "        strict = 'filter' \n",
    "\n",
    "\n",
    "class RawTrackingSchema(pa.DataFrameModel):\n",
    "    \"\"\"\n",
    "    Validates 'input_wXX.csv' files (Pre-Throw) frame.\n",
    "    \"\"\"\n",
    "    game_id: Series[int] = pa.Field(coerce=True)\n",
    "    play_id: Series[int] = pa.Field(coerce=True)\n",
    "    frame_id: Series[int] = pa.Field(coerce=True, ge=1)\n",
    "    nfl_id: Series[float] = pa.Field(coerce=True, nullable=True) # Nullable for Ball\n",
    "\n",
    "    play_direction: Series[str] \n",
    "    player_name: Series[str]\n",
    "    absolute_yardline_number: Series[int] = pa.Field(ge=0, le=120, nullable=True)\n",
    "    \n",
    "    player_role: Series[str] = pa.Field(nullable=True)\n",
    "    player_position: Series[str] = pa.Field(nullable=True)\n",
    "    \n",
    "    x: Series[float] = pa.Field(ge=0, nullable=True)\n",
    "    y: Series[float] = pa.Field(ge=0, nullable=True)\n",
    "    s: Series[float] = pa.Field(ge=0, nullable=True) \n",
    "    \n",
    "    ball_land_x: Series[float] = pa.Field(nullable=True)\n",
    "    ball_land_y: Series[float] = pa.Field(nullable=True)\n",
    "\n",
    "    class Config:\n",
    "        strict = 'filter' \n",
    "\n",
    "\n",
    "class OutputTrackingSchema(pa.DataFrameModel):\n",
    "    \"\"\"\n",
    "    Validates 'output_wXX.csv' files (Post-Throw) frame.\n",
    "    \"\"\"\n",
    "    game_id: Series[int] = pa.Field(coerce=True)\n",
    "    play_id: Series[int] = pa.Field(coerce=True)\n",
    "    nfl_id: Series[float] = pa.Field(coerce=True, nullable=True)\n",
    "    frame_id: Series[int] = pa.Field(coerce=True)\n",
    "    \n",
    "    x: Series[float] = pa.Field(ge=0, nullable=True)\n",
    "    y: Series[float] = pa.Field(ge=0, nullable=True)\n",
    "\n",
    "    class Config:\n",
    "        strict = 'filter'\n",
    "\n",
    "\n",
    "class PreprocessedSchema(RawTrackingSchema, RawSuppSchema):\n",
    "    \"\"\"\n",
    "    Validates the output of 'preprocessing.py'.\n",
    "    \"\"\"\n",
    "    phase: Series[str] = pa.Field(isin=[\"pre_throw\", \"post_throw\"])\n",
    "    yards_from_own_goal: Series[int] = pa.Field(ge=0, le=100, nullable=True)\n",
    "    possession_win_prob: Series[float] = pa.Field(ge=0, le=1, nullable=True)\n",
    "\n",
    "    class Config:\n",
    "        strict = 'filter'\n",
    "\n",
    "\n",
    "class PhysicsSchema(PreprocessedSchema):\n",
    "    \"\"\"\n",
    "    Validates the output of 'physics_engine.py'.\n",
    "    \"\"\"\n",
    "    s_derived: Series[float] = pa.Field(nullable=True, coerce=True) # Speed\n",
    "    a_derived: Series[float] = pa.Field(nullable=True, coerce=True) # Accel\n",
    "        \n",
    "    class Config:\n",
    "        strict = 'filter'\n",
    "\n",
    "\n",
    "class AggregationScoresSchema(pa.DataFrameModel):\n",
    "    \"\"\"\n",
    "    Validates the 'Score' subset merged onto the animation frames.\n",
    "    \"\"\"\n",
    "    game_id: Series[int] = pa.Field(coerce=True)\n",
    "    play_id: Series[int] = pa.Field(coerce=True)\n",
    "    nfl_id: Series[float] = pa.Field(coerce=True)\n",
    "\n",
    "    dist_at_throw: Series[float] = pa.Field(ge=0, nullable=True)\n",
    "    void_type: Series[str] = pa.Field(isin=[\"High Void\", \"Tight Window\", \"Neutral\"], nullable=True)\n",
    "\n",
    "    vis_score: Series[float] = pa.Field(nullable=True)  \n",
    "    ceoe_score: Series[float] = pa.Field(nullable=True)\n",
    "\n",
    "    class Config:\n",
    "        strict = 'filter'\n",
    "\n",
    "\n",
    "class FullPlayAnimationSchema(PhysicsSchema, AggregationScoresSchema): \n",
    "    \"\"\"\n",
    "    Validates the play animation frames.\n",
    "    \"\"\"\n",
    "    class Config:\n",
    "        strict = 'filter'\n",
    "\n",
    "\n",
    "class ContextSchema(pa.DataFrameModel):\n",
    "    \"\"\"\n",
    "    Validates the output of the ContextEngine.\n",
    "    \"\"\"\n",
    "    game_id: Series[int] = pa.Field(coerce=True)\n",
    "    play_id: Series[int] = pa.Field(coerce=True)\n",
    "    \n",
    "    target_nfl_id: Series[float] = pa.Field(nullable=True)\n",
    "    nearest_def_nfl_id: Series[float] = pa.Field(nullable=True)\n",
    "    dist_at_throw: Series[float] = pa.Field(ge=0, nullable=True) \n",
    "    void_type: Series[str] = pa.Field(isin=[\"High Void\", \"Tight Window\", \"Neutral\"], nullable=True)\n",
    "\n",
    "    class Config:\n",
    "        strict = 'filter'\n",
    "\n",
    "\n",
    "class EraserMetricsSchema(pa.DataFrameModel):\n",
    "    \"\"\"\n",
    "    Validates the the output of EraserEngine.\n",
    "    \"\"\"\n",
    "    game_id: Series[int] = pa.Field(coerce=True)\n",
    "    play_id: Series[int] = pa.Field(coerce=True)\n",
    "    nfl_id: Series[float] = pa.Field(coerce=True)\n",
    "    \n",
    "    p_dist_at_throw: Series[float] = pa.Field(ge=0, nullable=True)\n",
    "    dist_at_arrival: Series[float] = pa.Field() \n",
    "    distance_closed: Series[float] = pa.Field() \n",
    "    avg_closing_speed: Series[float] = pa.Field() \n",
    "    \n",
    "    vis_score: Series[float] = pa.Field() \n",
    "\n",
    "    class Config:\n",
    "        strict = 'filter'\n",
    "\n",
    "\n",
    "class BenchMarkingSchema(pa.DataFrameModel):\n",
    "    \"\"\"\n",
    "    Validates the Player Metadata (Static info) to be attached to the final report.\n",
    "    \"\"\"\n",
    "    game_id: Series[int] = pa.Field(coerce=True)\n",
    "    play_id: Series[int] = pa.Field(coerce=True)\n",
    "    nfl_id: Series[float] = pa.Field(coerce=True)\n",
    "    \n",
    "    player_role: Series[str] = pa.Field()\n",
    "    player_name: Series[str] = pa.Field(nullable=True)\n",
    "    player_position: Series[str] = pa.Field()\n",
    "    week: Series[int] = pa.Field(coerce=True) \n",
    "    down: Series[int] = pa.Field(coerce=True)\n",
    "    team_coverage_type: Series[str]\n",
    "    pass_result: Series[str] = pa.Field(nullable=True)\n",
    "    yards_gained: Series[int] = pa.Field(coerce=True, nullable=True)\n",
    "    pass_length: Series[int] = pa.Field(coerce=True, nullable=True)\n",
    "    expected_points_added: Series[float] = pa.Field(coerce=True, nullable=True)\n",
    "\n",
    "    class Config:\n",
    "        strict = 'filter'\n",
    "\n",
    "\n",
    "class AnalysisReportSchema(pa.DataFrameModel):\n",
    "    \"\"\"\n",
    "    Validates the Player Metadata (Static info) to be attached to the final report.\n",
    "    \"\"\"\n",
    "    game_id: Series[int]\n",
    "    play_id: Series[int]\n",
    "    nfl_id: Series[float]\n",
    "    \n",
    "    player_position: Series[str] = pa.Field(nullable=False)\n",
    "    player_name: Series[str] = pa.Field(nullable=True)\n",
    "    player_role: Series[str]\n",
    "    team_coverage_type: Series[str]\n",
    "    down: Series[int]\n",
    "    pass_result: Series[str] = pa.Field(nullable=True)\n",
    "    dist_at_throw: Series[float] = pa.Field(ge=0, nullable=True)\n",
    "    dist_at_arrival: Series[float] = pa.Field(ge=0, nullable=True)\n",
    "    yards_gained: Series[int] = pa.Field(coerce=True, nullable=True)\n",
    "    pass_length: Series[int] = pa.Field(coerce=True, nullable=True)\n",
    "    expected_points_added: Series[float] = pa.Field(coerce=True, nullable=True)\n",
    "\n",
    "    void_type: Series[str] = pa.Field(isin=[\"High Void\", \"Tight Window\", \"Neutral\"], nullable=True)\n",
    "    \n",
    "    vis_score: Series[float]\n",
    "    avg_closing_speed: Series[float]\n",
    "    p_dist_at_throw: Series[float] = pa.Field(ge=0, nullable=True)\n",
    "\n",
    "    ceoe_score: Series[float] = pa.Field(nullable=False)\n",
    "\n",
    "    class Config:\n",
    "        strict = 'filter' \n",
    "\n",
    "class EraserEngine:\n",
    "    def __init__(self):\n",
    "        self.output_schema = EraserMetricsSchema\n",
    "\n",
    "    def calculate_eraser(self, df: pd.DataFrame, context_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculates how distinct defenders close space on the targeted receiver.\n",
    "        \"\"\"\n",
    "        # Filter for Post-Throw Phase only\n",
    "        df_post = df[df['phase'] == 'post_throw'].copy()\n",
    "        \n",
    "        # Isolate the Targeted Receiver's path\n",
    "        # We need the receiver's X,Y for every frame to compare against defenders\n",
    "        targets = df_post[df_post['player_role'] == 'Targeted Receiver'][\n",
    "            ['game_id', 'play_id', 'frame_id', 'x', 'y']\n",
    "        ].rename(columns={'x': 't_x', 'y': 't_y'})\n",
    "\n",
    "        # Isolate Defenders\n",
    "        defenders = df_post[df_post['player_role'] == 'Defensive Coverage'][\n",
    "            ['game_id', 'play_id', 'nfl_id', 'frame_id', 'x', 'y']\n",
    "        ]\n",
    "\n",
    "        # Merge Defender + Target on (Game, Play, Frame)\n",
    "        merged = defenders.merge(targets, on=['game_id', 'play_id', 'frame_id'], how='inner')\n",
    "\n",
    "        # Calculate Dynamic Separation (Distance to Target)\n",
    "        merged['dist_to_target'] = np.sqrt(\n",
    "            (merged['x'] - merged['t_x'])**2 + \n",
    "            (merged['y'] - merged['t_y'])**2\n",
    "        )\n",
    "\n",
    "        def grade_defender(group):\n",
    "            group = group.sort_values('frame_id')\n",
    "\n",
    "            d_start = group['dist_to_target'].iloc[0] # Distance at Throw\n",
    "            d_end = group['dist_to_target'].iloc[-1]  # Distance at Arrival\n",
    "            \n",
    "            # Metric 1: VIS (Void Improvement Score)\n",
    "            # Positive = Good (Closed gap), Negative = Bad (Lost gap)\n",
    "            vis = d_start - d_end\n",
    "            \n",
    "            # Metric 2: Closing Speed (Rate of Change)\n",
    "            # Calculate distance change per frame\n",
    "            # We multiply by -1 because getting closer (dist going down) is positive speed\n",
    "            dist_change = group['dist_to_target'].diff() * -1\n",
    "            \n",
    "            # Convert to Yards/Second (1 frame = 0.1s)\n",
    "            speeds = dist_change * 10 \n",
    "            avg_speed = speeds.mean()\n",
    "            \n",
    "            return pd.Series({\n",
    "                'p_dist_at_throw': d_start,\n",
    "                'dist_at_arrival': d_end,\n",
    "                'distance_closed': max(0, vis),\n",
    "                'vis_score': vis,\n",
    "                'avg_closing_speed': avg_speed\n",
    "            })\n",
    "\n",
    "        # Apply grouping per player per play\n",
    "        metrics = merged.groupby(['game_id', 'play_id', 'nfl_id']).apply(grade_defender).reset_index()\n",
    "\n",
    "        return self.output_schema.validate(metrics)\n",
    "\n",
    "\n",
    "class BenchmarkingEngine:\n",
    "    \"\"\"\n",
    "    benchmarking defender performance using context and physics data.\n",
    "    Calculates CEOE (Closing Efficiency Over Expectation) for each play/defender.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.bench_schema = BenchMarkingSchema\n",
    "        self.report_schema = AnalysisReportSchema\n",
    "\n",
    "    def calculate_ceoe(self, df_metrics: pd.DataFrame, \n",
    "                       df_context: pd.DataFrame, df_physics: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate CEOE (Closing Efficiency Over Expectation) for defenders.\n",
    "        CEOE = Player's avg closing speed - Positional/contextual average.\n",
    "        \"\"\"\n",
    "        meta_cols = list(self.bench_schema.to_schema().columns.keys())\n",
    "        df_meta = self.bench_schema.validate(df_physics[meta_cols])\n",
    "        df_meta = df_meta.drop_duplicates()\n",
    "\n",
    "        df_final = df_metrics.merge(df_meta, on=['game_id', 'play_id', 'nfl_id'], how='left')\n",
    "        df_final = df_final.merge(\n",
    "            df_context[['game_id', 'play_id', 'void_type', 'dist_at_throw']], \n",
    "            on=['game_id', 'play_id'], \n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Calculate positional/contextual average closing speed\n",
    "        benchmarks = df_final.groupby(\n",
    "            ['player_position', 'void_type'])['avg_closing_speed'].transform('mean')\n",
    "\n",
    "        df_final['ceoe_score'] = df_final['avg_closing_speed'] - benchmarks\n",
    "        df_final['ceoe_score'] = df_final['ceoe_score'].fillna(0.0)\n",
    "        \n",
    "        return self.report_schema.validate(df_final)\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, data_dir: str, supp_file: str):\n",
    "        \"\"\"\n",
    "        Scans the directory for files but DOES NOT load them yet.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.supp_file = supp_file\n",
    "\n",
    "        self.input_files = sorted(glob.glob(os.path.join(self.data_dir, 'input_*.csv')))\n",
    "        self.output_files = glob.glob(os.path.join(self.data_dir, 'output_*.csv'))\n",
    "        \n",
    "        self.output_map = {}\n",
    "        for f in self.output_files:\n",
    "            match = re.search(r'w(\\d{2})', f)\n",
    "            if not match:\n",
    "                continue\n",
    "            self.output_map[match.group(1)] = f\n",
    "\n",
    "    def load_supplementary(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads the single Supplementary file.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.supp_file):\n",
    "            raise FileNotFoundError(f\"Missing Supp File: {self.supp_file}\")\n",
    "            \n",
    "        df = pd.read_csv(self.supp_file, low_memory=False)\n",
    "            \n",
    "        return RawSuppSchema.validate(df)\n",
    "\n",
    "    def stream_weeks(self) -> Generator[Tuple[str, pd.DataFrame, pd.DataFrame], None, None]:\n",
    "        \"\"\"\n",
    "        The Lazy Loader.\n",
    "        Yields: (week_num, input_df, output_df)\n",
    "        \"\"\"\n",
    "\n",
    "        for input_path in self.input_files:\n",
    "\n",
    "            # Extract Week Number\n",
    "            match = re.search(r'w(\\d{2})', input_path)\n",
    "            \n",
    "            if not match: continue\n",
    "            week_num = match.group(1)\n",
    "            \n",
    "            output_path = self.output_map.get(week_num)\n",
    "\n",
    "            print(f\"Streaming Week {week_num}...\")\n",
    "            \n",
    "            # Load from Disk\n",
    "            input_raw = pd.read_csv(input_path, low_memory=False)\n",
    "            output_raw = pd.read_csv(output_path, low_memory=False)\n",
    "            \n",
    "            input_raw['nfl_id'] = pd.to_numeric(input_raw['nfl_id'], errors='coerce')\n",
    "            output_raw['nfl_id'] = pd.to_numeric(output_raw['nfl_id'], errors='coerce')\n",
    "\n",
    "            # VALIDATE\n",
    "            input_valid = RawTrackingSchema.validate(input_raw)\n",
    "            output_valid = OutputTrackingSchema.validate(output_raw)\n",
    "            \n",
    "            # Yield the clean, validated data to the Orchestrator\n",
    "            yield week_num, input_valid, output_valid\n",
    "\n",
    "\n",
    "class DataExporter:\n",
    "    def __init__(self, output_dir: str):\n",
    "        self.output_dir = output_dir\n",
    "        self.report_schema = AnalysisReportSchema\n",
    "        self.animation_schema = AggregationScoresSchema\n",
    "        self.full_animation = FullPlayAnimationSchema\n",
    "\n",
    "    def export_results(self, df_summary: pd.DataFrame, df_frames: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        1. Validates & Saves the Analytical Report.\n",
    "        2. Validates & Merges Scores for Animation.\n",
    "        3. Saves the Master Animation File.\n",
    "        \"\"\"\n",
    "        print(f\"   -> Output Directory: {self.output_dir}\")\n",
    "\n",
    "        self.report_schema.validate(df_summary)\n",
    "\n",
    "        summary_path = os.path.join(self.output_dir, 'eraser_analysis_summary.csv')\n",
    "        df_summary.to_csv(summary_path, index=False)\n",
    "        print(f\"   -> Saved Eraser Analysis Report to {summary_path}\")\n",
    "\n",
    "        # Define the subset of columns to attach to the visualizer\n",
    "        score_cols = list(self.animation_schema.to_schema().columns.keys())\n",
    "        flags_to_merge = self.animation_schema.validate(df_summary[score_cols])\n",
    "\n",
    "        # MERGE: Left join the scores onto the massive physics dataframe\n",
    "        # This repeats the score for every frame of the play\n",
    "        df_animation = df_frames.merge(\n",
    "            flags_to_merge, \n",
    "            on=['game_id', 'play_id', 'nfl_id'], \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        self.full_animation.validate(df_animation)\n",
    "\n",
    "        final_path = os.path.join(self.output_dir, 'master_animation_data.csv')\n",
    "        df_animation.to_csv(final_path, index=False)\n",
    "        \n",
    "        print(f\"   -> Saved Animation Master File to {final_path}\")\n",
    "\n",
    "\n",
    "class PhysicsEngine:\n",
    "    def __init__(self):\n",
    "        self.output_schema = PhysicsSchema\n",
    "\n",
    "    def derive_metrics(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Applies Savitzky-Golay filter to calculate generic Speed (s) and Acceleration (a).\n",
    "        REMOVED: Direction (dir) calculation, as we now use specific vectors in Phase B.\n",
    "        \"\"\"\n",
    "        # Ensure temporal ordering for the filter\n",
    "        df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "        \n",
    "        # SAVITZKY-GOLAY PARAMETERS\n",
    "        WINDOW = 7 # 0.7 seconds\n",
    "        POLY = 2   # Quadratic fit\n",
    "        \n",
    "        def calculate_sg(group):\n",
    "\n",
    "            if len(group) < WINDOW:\n",
    "                dx = group['x'].diff() \n",
    "                dy = group['y'].diff()\n",
    "                \n",
    "                dist = np.sqrt(dx**2 + dy**2)\n",
    "                s = dist / 0.1 \n",
    "                \n",
    "                # Acceleration is diff of speed\n",
    "                a = s.diff().fillna(0) / 0.1\n",
    "                \n",
    "                return pd.DataFrame(\n",
    "                    {'s_derived': s, 'a_derived': a}, \n",
    "                    index=group.index)\n",
    "            \n",
    "            # First Derivative (Velocity)\n",
    "            vx = savgol_filter(group['x'], window_length=WINDOW, polyorder=POLY, deriv=1, delta=0.1)\n",
    "            vy = savgol_filter(group['y'], window_length=WINDOW, polyorder=POLY, deriv=1, delta=0.1)\n",
    "            \n",
    "            # Second Derivative (Acceleration)\n",
    "            ax = savgol_filter(group['x'], window_length=WINDOW, polyorder=POLY, deriv=2, delta=0.1)\n",
    "            ay = savgol_filter(group['y'], window_length=WINDOW, polyorder=POLY, deriv=2, delta=0.1)\n",
    "            \n",
    "            # Magnitudes (Scalar)\n",
    "            s = np.sqrt(vx**2 + vy**2)\n",
    "            a = np.sqrt(ax**2 + ay**2)\n",
    "            \n",
    "            return pd.DataFrame({\n",
    "                's_derived': s,\n",
    "                'a_derived': a\n",
    "            }, index=group.index)\n",
    "\n",
    "        # Apply grouping\n",
    "        # Only apply to players (nfl_id is not null)\n",
    "        mask_players = df['nfl_id'].notna()\n",
    "        \n",
    "        physics_cols = df[mask_players].groupby(\n",
    "            ['game_id', 'play_id', 'nfl_id'], group_keys=False).apply(calculate_sg, include_groups=False)\n",
    "\n",
    "        # Map back to original DataFrame\n",
    "        df.loc[physics_cols.index, 's_derived'] = physics_cols['s_derived']\n",
    "        df.loc[physics_cols.index, 'a_derived'] = physics_cols['a_derived']\n",
    "\n",
    "        return self.output_schema.validate(df)\n",
    "\n",
    "class DataPreProcessor:\n",
    "    def __init__(self):\n",
    "        self.output_schema = PreprocessedSchema\n",
    "        self.keep_cols = list(self.output_schema.to_schema().columns.keys())\n",
    "\n",
    "    def filter_context(self, supp_df):\n",
    "        \"\"\"\n",
    "        Filters the supplementary dataframe and performs 'Lightweight Feature Engineering'..\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate Possession Win Probability\n",
    "        supp_df['possession_win_prob'] = np.where(\n",
    "            supp_df['possession_team'] == supp_df['home_team_abbr'],\n",
    "            supp_df['pre_snap_home_team_win_probability'],\n",
    "            supp_df['pre_snap_visitor_team_win_probability'],\n",
    "        )\n",
    "        \n",
    "        # Calculate Normalized Field Position (0-100 Scale)\n",
    "        # Logic: If on own side, use number. If on opp side, use 100 - number.\n",
    "        supp_df['yards_from_own_goal'] = np.where(\n",
    "            supp_df['yardline_side'] == supp_df['possession_team'],\n",
    "            supp_df['yardline_number'],           \n",
    "            100 - supp_df['yardline_number']      \n",
    "        )\n",
    "\n",
    "        # valid mask filters\n",
    "        valid_mask = (\n",
    "            (supp_df['team_coverage_man_zone'].astype(str).str.contains('Zone', case=False, na=False)) &\n",
    "            (supp_df['pass_result'].isin(['C', 'I', 'IN'])) &\n",
    "            (supp_df['team_coverage_type'] != 'COVER_6_ZONE') &\n",
    "            (~supp_df['dropback_type'].str.upper().isin([\n",
    "                'SCRAMBLE', 'SCRAMBLE_ROLLOUT_LEFT', 'SCRAMBLE_ROLLOUT_RIGHT', 'QB_DRAW'])) &\n",
    "            (supp_df['play_nullified_by_penalty'] != 'Y')\n",
    "        )\n",
    "\n",
    "        # remove trick / cheap plays\n",
    "        screen_shovel_mask = (\n",
    "            supp_df['route_of_targeted_receiver'].astype(str).str.upper().str.contains('SCREEN', na=False) | \n",
    "            \n",
    "            # Physics Check: Ball caught behind or at LOS (Shovels/Swings)\n",
    "            (supp_df['pass_length'] <= 0) | \n",
    "            \n",
    "            # Check-downs (Flat routes < 3 yards)\n",
    "            (\n",
    "                (supp_df['route_of_targeted_receiver'].astype(str).str.upper() == 'FLAT') & \n",
    "                (supp_df['pass_length'] < 3)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # base situations\n",
    "        base_situation_mask = (           \n",
    "            (supp_df['down'].isin([1, 2])) &\n",
    "            \n",
    "            # Competitive Game (Neutral Script)\n",
    "            (supp_df['possession_win_prob'].between(0.20, 0.80)) & \n",
    "\n",
    "            # On Schedule (Not 1st & 20 or 2nd & 18)\n",
    "            (supp_df['yards_to_go'] <= 10) &\n",
    "            \n",
    "            # We use our new engineered column here\n",
    "            (supp_df['yards_from_own_goal'].between(20, 80))\n",
    "        )\n",
    "        \n",
    "        final_valid_mask = (\n",
    "            valid_mask &\n",
    "            (~screen_shovel_mask) &\n",
    "            base_situation_mask\n",
    "        )\n",
    "\n",
    "        return supp_df[final_valid_mask].copy()\n",
    "\n",
    "    def _stitch_tracking_data(self, input_df, output_df, valid_keys):\n",
    "        \"\"\"\n",
    "        Pure Logic. Merges Pre-Throw and Post-Throw data.\n",
    "        \"\"\"\n",
    "        # Filter Input\n",
    "        input_df['key_tuple'] = list(zip(input_df.game_id, input_df.play_id))\n",
    "        input_df = input_df[input_df['key_tuple'].isin(valid_keys)].drop(columns=['key_tuple'])\n",
    "        input_df['phase'] = 'pre_throw'\n",
    "        \n",
    "        if output_df.empty: return input_df\n",
    "\n",
    "        # Filter Output\n",
    "        output_df['key_tuple'] = list(zip(output_df.game_id, output_df.play_id))\n",
    "        output_df = output_df[output_df['key_tuple'].isin(valid_keys)].drop(columns=['key_tuple'])\n",
    "        \n",
    "        # Logic: Tag first frame of Output as pass_forward\n",
    "        output_df['event'] = None\n",
    "        output_df.loc[output_df['frame_id'] == 1, 'event'] = 'pass_forward'\n",
    "        \n",
    "        # TODO: move this to schema\n",
    "        # Metadata Propagation (Players missing in Output get this from Input)\n",
    "        meta_cols = ['game_id', 'play_id', 'nfl_id', 'player_name', 'jersey_number', 'player_position', \n",
    "                     'player_role', 'player_side', 'play_direction', 'absolute_yardline_number', \n",
    "                     'ball_land_x', 'ball_land_y']\n",
    "        \n",
    "        avail_cols = [c for c in meta_cols if c in input_df.columns]\n",
    "        player_meta = input_df[avail_cols].drop_duplicates(subset=['game_id', 'play_id', 'nfl_id'])\n",
    "        \n",
    "        # Frame Offset Calculation\n",
    "        play_offsets = input_df.groupby(['game_id', 'play_id'])['frame_id'].max().reset_index()\n",
    "        play_offsets.columns = ['game_id', 'play_id', 'offset']\n",
    "        \n",
    "        output_df = output_df.merge(player_meta, on=['game_id', 'play_id', 'nfl_id'], how='left')\n",
    "        output_df = output_df.merge(play_offsets, on=['game_id', 'play_id'], how='left')\n",
    "        \n",
    "        # Apply Offset\n",
    "        output_df['frame_id'] = output_df['frame_id'] + output_df['offset'].fillna(0)\n",
    "        output_df['phase'] = 'post_throw'\n",
    "        \n",
    "        df = pd.concat([input_df, output_df.drop(columns=['offset'])], ignore_index=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _normalize_coordinates(self, df):\n",
    "        \"\"\"\n",
    "        Standardizes field geometry to Left->Right drive direction.\n",
    "        \"\"\"\n",
    "        if 'play_direction' not in df.columns: return df\n",
    "        mask = df['play_direction'].str.lower() == 'left'\n",
    "        \n",
    "        for col in ['x', 'ball_land_x']:\n",
    "            if col in df.columns: df.loc[mask, col] = 120 - df.loc[mask, col]\n",
    "\n",
    "        for col in ['y', 'ball_land_y']:\n",
    "            if col in df.columns: df.loc[mask, col] = 53.3 - df.loc[mask, col]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _clean_and_deduplicate(self, df):\n",
    "        \"\"\"\n",
    "        Ensures strict temporal ordering and removes duplicate frames at the stitch point.\n",
    "        \"\"\"\n",
    "        df['phase_rank'] = df['phase'].apply(lambda x: 1 if x == 'pre_throw' else 2)        \n",
    "        \n",
    "        df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id', 'phase_rank'])        \n",
    "        \n",
    "        df = df.drop_duplicates(subset=['game_id', 'play_id', 'nfl_id', 'frame_id'], keep='last')\n",
    "        \n",
    "        return df.drop(columns=['phase_rank'])\n",
    "\n",
    "    def process_single_week(self, week_num, input_df, output_df, context_df):\n",
    "        \"\"\"\n",
    "        Internal logic for a single week.\n",
    "        \"\"\"\n",
    "        valid_keys = set(zip(context_df.game_id, context_df.play_id))\n",
    "\n",
    "        week_df = self._stitch_tracking_data(input_df, output_df, valid_keys)\n",
    "\n",
    "        week_df = week_df.merge(context_df, on=['game_id', 'play_id'], how='inner')\n",
    "\n",
    "        week_df = self._normalize_coordinates(week_df)\n",
    "\n",
    "        week_df['los_x'] = week_df['ball_land_x'] - week_df['pass_length']\n",
    "        week_df['week'] = int(week_num)\n",
    "\n",
    "        week_df = self._clean_and_deduplicate(week_df)\n",
    "\n",
    "        return self.output_schema.validate(week_df)\n",
    "\n",
    "    def run(self, data_stream: Generator[Tuple[str, pd.DataFrame, pd.DataFrame], None, None], \n",
    "            raw_context_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        MAIN ENTRY POINT.\n",
    "        \"\"\"\n",
    "        clean_context = self.filter_context(raw_context_df)\n",
    "        \n",
    "        processed_chunks: List[pd.DataFrame] = []\n",
    "        for week_num, input_df, output_df in data_stream:\n",
    "            \n",
    "            clean_week_df = self.process_single_week(week_num, input_df, output_df, clean_context)\n",
    "\n",
    "            if not clean_week_df.empty:\n",
    "                processed_chunks.append(clean_week_df)\n",
    "\n",
    "            del input_df, output_df\n",
    "            gc.collect()\n",
    "\n",
    "        if not processed_chunks:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        return pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "def run_data_pipeline(DATA_DIR=None, SUPP_FILE=None, OUTPUT_DIR=None):\n",
    "    start_time = datetime.now()\n",
    "    print(\"hello\")\n",
    "    # Use provided arguments, else fall back to config.py values\n",
    "    cfg = DataPipelineConfig(\n",
    "        DATA_DIR=DATA_DIR or data_config.DATA_DIR,\n",
    "        SUPP_FILE=SUPP_FILE or data_config.SUPP_FILE,\n",
    "        OUTPUT_DIR=OUTPUT_DIR or data_config.OUTPUT_DIR\n",
    "    )\n",
    "\n",
    "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # 1. LOAD\n",
    "    print(f\"[1/7] Initializing Data Loader ({datetime.now().strftime('%H:%M:%S')})...\")\n",
    "    loader = DataLoader(cfg.DATA_DIR, cfg.SUPP_FILE)\n",
    "    raw_supp = loader.load_supplementary()\n",
    "    raw_tracking = loader.stream_weeks()\n",
    "\n",
    "    # 2. PREPROCESS\n",
    "    print(\"[2/7] Preprocessing & Stitching frames...\")\n",
    "    processor = DataPreProcessor()\n",
    "    df_clean = processor.run(data_stream=raw_tracking, raw_context_df=raw_supp)\n",
    "\n",
    "    # 3. PHYSICS\n",
    "    print(\"[3/7] Running Physics Engine (Kinematics)...\")\n",
    "    physics_engine = PhysicsEngine()\n",
    "    df_physics = physics_engine.derive_metrics(df_clean)\n",
    "    \n",
    "    del df_clean\n",
    "    gc.collect() \n",
    "\n",
    "    # 4. CONTEXT\n",
    "    # TODO: Note this is changing the dataframe entirely - df_physics is our animation dataset.\n",
    "    print(\"[4/7] Phase A: Calculating Void Context (S_throw)...\")\n",
    "    context_engine = ContextEngine()\n",
    "    df_context = context_engine.calculate_void_context(df_physics)\n",
    "    \n",
    "    # Debugging\n",
    "    print(f\"   -> Identified Voids for {df_context.shape[0]} plays.\")\n",
    "\n",
    "    # 5. ERASER\n",
    "    print(\"[5/7] Phase B: Calculating Eraser Metrics (VIS)...\")\n",
    "    eraser_engine = EraserEngine()\n",
    "    df_metrics = eraser_engine.calculate_eraser(df_physics, df_context)\n",
    "\n",
    "    # 6. BENCHMARKING\n",
    "    print(\"[6/7] Phase C: Benchmarking (CEOE)...\")\n",
    "    benchmarker = BenchmarkingEngine()\n",
    "    df_final = benchmarker.calculate_ceoe(\n",
    "        df_metrics=df_metrics, \n",
    "        df_context=df_context, \n",
    "        df_physics=df_physics\n",
    "    )\n",
    "\n",
    "    # 7. EXPORT\n",
    "    print(\"[7/7] Phase D: Exporting Results...\")\n",
    "    exporter = DataExporter(cfg.OUTPUT_DIR)\n",
    "    exporter.export_results(\n",
    "        df_summary=df_final, \n",
    "        df_frames=df_physics\n",
    "    )\n",
    "    \n",
    "    duration = datetime.now() - start_time\n",
    "    print(f\"PIPELINE FINISHED in {duration}\")\n",
    "\n",
    "class ContextEngine:\n",
    "    def __init__(self):\n",
    "        self.output_schema = ContextSchema\n",
    "\n",
    "    def calculate_void_context(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        capture ALL players at the moment of the throw.\n",
    "        \"\"\"\n",
    "        # Filter for Pre-Throw phase\n",
    "        pre_throw_mask = df['phase'] == 'pre_throw'\n",
    "        df_pre = df[pre_throw_mask].copy()\n",
    "\n",
    "        # we calculate the MAX frame_id for every play\n",
    "        last_frame_ids = df_pre.groupby(['game_id', 'play_id'])['frame_id'].transform('max')\n",
    "        \n",
    "        # We keep rows where the frame_id matches the last frame of that play\n",
    "        throw_frames = df_pre[df_pre['frame_id'] == last_frame_ids].copy()\n",
    "\n",
    "        # Split into Target vs. Defenders\n",
    "        targets = throw_frames[throw_frames['player_role'].astype(str).str.strip() == 'Targeted Receiver'][\n",
    "            ['game_id', 'play_id', 'nfl_id', 'x', 'y', 'week']\n",
    "        ].rename(columns={'nfl_id': 'target_nfl_id', 'x': 't_x', 'y': 't_y'})\n",
    "\n",
    "        defenders = throw_frames[throw_frames['player_role'].astype(str).str.strip() == 'Defensive Coverage'][\n",
    "            ['game_id', 'play_id', 'nfl_id', 'x', 'y']\n",
    "        ].rename(columns={'nfl_id': 'def_nfl_id', 'x': 'd_x', 'y': 'd_y'})\n",
    "\n",
    "        merged = defenders.merge(targets, on=['game_id', 'play_id'], how='inner')\n",
    "\n",
    "        merged['dist'] = np.sqrt(\n",
    "            (merged['d_x'] - merged['t_x'])**2 + \n",
    "            (merged['d_y'] - merged['t_y'])**2\n",
    "        )\n",
    "\n",
    "        # Find the Nearest Neighbor\n",
    "        min_dists = merged.loc[merged.groupby(['game_id', 'play_id'])['dist'].idxmin()]\n",
    "\n",
    "        context_df = min_dists[['game_id', 'play_id', 'week', 'target_nfl_id', 'def_nfl_id', 'dist']].copy()\n",
    "        \n",
    "        context_df = context_df.rename(columns={\n",
    "            'def_nfl_id': 'nearest_def_nfl_id', \n",
    "            'dist': 'dist_at_throw'\n",
    "        })\n",
    "\n",
    "        # Apply Classification Labels\n",
    "        conditions = [\n",
    "            (context_df['dist_at_throw'] > 5.0),\n",
    "            (context_df['dist_at_throw'] < 2.0)\n",
    "        ]\n",
    "        choices = ['High Void', 'Tight Window']\n",
    "        context_df['void_type'] = np.select(conditions, choices, default='Neutral')\n",
    "\n",
    "        return self.output_schema.validate(context_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c8577b-0e5d-4f3a-8135-018751349644",
   "metadata": {},
   "source": [
    "### Start Pipeline\n",
    "say what the output will be + read the new datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e364378-0813-4917-895b-edd6204bae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76fe7cd-7dbd-408e-aebb-a55057c7f0a6",
   "metadata": {},
   "source": [
    "## Visualization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5385ba6e-cd2a-4c36-b910-55c36e969a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NFL_TEAM_COLORS = {\n",
    "    'BAL': {'primary': '#241773', 'secondary': '#000000', 'alternate': '#9E7C0C'},\n",
    "    'CIN': {'primary': '#FB4F14', 'secondary': '#000000'},\n",
    "    'CLE': {'primary': '#311D00', 'secondary': '#FF3C00'},\n",
    "    'PIT': {'primary': '#FFB612', 'secondary': '#101820', 'alternate': '#003087'},\n",
    "    'BUF': {'primary': '#00338D', 'secondary': '#C60C30'},\n",
    "    'MIA': {'primary': '#008E97', 'secondary': '#FC4C02', 'alternate': '#005778'},\n",
    "    'NE': {'primary': '#002244', 'secondary': '#C60C30', 'alternate': '#B0B7BC'},\n",
    "    'NYJ': {'primary': '#125740', 'secondary': '#000000', 'alternate': '#FFFFFF'},\n",
    "    'HOU': {'primary': '#03202F', 'secondary': '#A71930'},\n",
    "    'IND': {'primary': '#002C5F', 'secondary': '#A2AAAD'},\n",
    "    'JAX': {'primary': '#101820', 'secondary': '#D7A22A', 'alternate': '#9F792C'},\n",
    "    'TEN': {'primary': '#0C2340', 'secondary': '#4B92DB', 'alternate': '#C8102E'},\n",
    "    'DEN': {'primary': '#FB4F14', 'secondary': '#002244'},\n",
    "    'KC': {'primary': '#E31837', 'secondary': '#FFB81C'},\n",
    "    'LV': {'primary': '#000000', 'secondary': '#A5ACAF'},\n",
    "    'LAC': {'primary': '#0080C6', 'secondary': '#FFC20E', 'alternate': '#FFFFFF'},\n",
    "    'CHI': {'primary': '#0B162A', 'secondary': '#C83803'},\n",
    "    'DET': {'primary': '#0076B6', 'secondary': '#B0B7BC', 'alternate': '#000000'},\n",
    "    'GB': {'primary': '#203731', 'secondary': '#FFB612'},\n",
    "    'MIN': {'primary': '#4F2683', 'secondary': '#FFC62F'},\n",
    "    'DAL': {'primary': '#003594', 'secondary': '#041E42', 'alternate': '#869397'},\n",
    "    'NYG': {'primary': '#0B2265', 'secondary': '#A71930', 'alternate': '#A5ACAF'},\n",
    "    'PHI': {'primary': '#004C54', 'secondary': '#A5ACAF', 'alternate': '#000000'},\n",
    "    'WAS': {'primary': '#5A1414', 'secondary': '#FFB612'},\n",
    "    'ATL': {'primary': '#A71930', 'secondary': '#000000', 'alternate': '#A5ACAF'},\n",
    "    'CAR': {'primary': '#0085CA', 'secondary': '#101820', 'alternate': '#BFC0BF'},\n",
    "    'NO': {'primary': '#D3BC8D', 'secondary': '#101820'},\n",
    "    'TB': {'primary': '#D50A0A', 'secondary': '#FF7900', 'alternate': '#B1BABF'},\n",
    "    'ARI': {'primary': '#97233F', 'secondary': '#000000', 'alternate': '#FFB612'},\n",
    "    'LAR': {'primary': '#003594', 'secondary': '#FFA300', 'alternate': '#FF8200'},\n",
    "    'SF': {'primary': '#AA0000', 'secondary': '#B3995D'},\n",
    "    'SEA': {'primary': '#002244', 'secondary': '#69BE28', 'alternate': '#A5ACAF'}\n",
    "}\n",
    "\n",
    "class AnimationEngine:\n",
    "    def __init__(self, summary_df, frames_df, output_dir):\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "       \n",
    "        self.summary_df = summary_df\n",
    "        self.frames_df = frames_df\n",
    "\n",
    "    def _draw_field(self, ax):\n",
    "        \"\"\"Sets up the static NFL-style field background with enhanced details.\"\"\"\n",
    "        ax.set_xlim(0, 120)\n",
    "        ax.set_ylim(0, 53.3)\n",
    "        ax.set_facecolor('#2e7d32')  \n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "       \n",
    "        # Field outline with thicker sidelines\n",
    "        field_rect = patches.Rectangle((10, 0), 100, 53.3, linewidth=3,\n",
    "                                         edgecolor='white', facecolor='#3d8b40', zorder=0)\n",
    "        ax.add_patch(field_rect)\n",
    "       \n",
    "        # End Zones with diagonal stripes for texture\n",
    "        ez_left = patches.Rectangle((0, 0), 10, 53.3, facecolor='#1b5e20', edgecolor='white', linewidth=2, zorder=0)\n",
    "        ez_right = patches.Rectangle((110, 0), 10, 53.3, facecolor='#1b5e20', edgecolor='white', linewidth=2, zorder=0)\n",
    "        ax.add_patch(ez_left)\n",
    "        ax.add_patch(ez_right)\n",
    "        # Add subtle diagonal lines in end zones for NFL feel\n",
    "        for y in np.arange(0, 53.3, 2):\n",
    "            ax.plot([0, 10], [y, y+2], color='white', alpha=0.1, lw=1)\n",
    "            ax.plot([110, 120], [y, y+2], color='white', alpha=0.1, lw=1)\n",
    "        ax.text(5, 26.65, 'END\\nZONE', ha='center', va='center', fontsize=8,\n",
    "                fontweight='bold', color='white', alpha=0.6, rotation=90)\n",
    "        ax.text(115, 26.65, 'END\\nZONE', ha='center', va='center', fontsize=8,\n",
    "                fontweight='bold', color='white', alpha=0.6, rotation=270)\n",
    "       \n",
    "        # Yard lines (every 5 yards, bold every 10)\n",
    "        for x in range(10, 111, 5):\n",
    "            lw = 3 if x % 10 == 0 else 1\n",
    "            alpha = 1.0 if x % 10 == 0 else 0.7\n",
    "            ax.axvline(x, color='white', linestyle='-', linewidth=lw, alpha=alpha, zorder=1)\n",
    "       \n",
    "        # Yard numbers with rotation for sideline view\n",
    "        for x in range(20, 110, 10):\n",
    "            num = (x - 10) if x <= 60 else (110 - x)\n",
    "            # Top numbers (rotated)\n",
    "            ax.text(x, 48, str(num), color='white', ha='center', va='center',\n",
    "                    fontsize=14, fontweight='bold', alpha=0.8, rotation=180)\n",
    "            # Bottom numbers\n",
    "            ax.text(x, 5, str(num), color='white', ha='center', va='center',\n",
    "                    fontsize=14, fontweight='bold', alpha=0.8)\n",
    "       \n",
    "        # Hash marks (NFL style, more accurate spacing)\n",
    "        hash_y_top = 39.3  \n",
    "        hash_y_bot = 14.0\n",
    "        for x in range(10, 111, 1):\n",
    "            ax.plot([x, x], [hash_y_top, hash_y_top + 0.5], color='white', linewidth=1, alpha=0.8, zorder=1)\n",
    "            ax.plot([x, x], [hash_y_bot - 0.5, hash_y_bot], color='white', linewidth=1, alpha=0.8, zorder=1)\n",
    "       \n",
    "        # Add NFL logo at center for elite feel\n",
    "        ax.text(60, 26.65, 'NFL', ha='center', va='center', fontsize=24, fontweight='bold',\n",
    "                color='white', alpha=0.15)\n",
    "        # Goal posts (simplified Y-shape projection)\n",
    "        gp_color = '#FFD700'  \n",
    "       \n",
    "        # Left goal post\n",
    "        ax.plot([0, 0], [23.65, 29.65], color=gp_color, lw=4)  \n",
    "        ax.plot([-1, 1], [26.65, 26.65], color=gp_color, lw=4)\n",
    "       \n",
    "        # Right goal post\n",
    "        ax.plot([120, 120], [23.65, 29.65], color=gp_color, lw=4)\n",
    "        ax.plot([119, 121], [26.65, 26.65], color=gp_color, lw=4)\n",
    "\n",
    "    def generate_video(self, game_id, play_id, eraser_id, filename=\"play_animation.mp4\"):\n",
    "        print(f\"   [Animator] Rendering video for {game_id}-{play_id}...\")\n",
    "       \n",
    "        # Get Play Data\n",
    "        play_frames = self.frames_df[\n",
    "            (self.frames_df['game_id'] == game_id) &\n",
    "            (self.frames_df['play_id'] == play_id)\n",
    "        ].sort_values('frame_id')\n",
    "       \n",
    "        if play_frames.empty: return\n",
    "\n",
    "        # Actors & Context\n",
    "        target_row = play_frames[play_frames['player_role'] == 'Targeted Receiver']\n",
    "        target_id = target_row['nfl_id'].iloc[0]\n",
    "        target_name = target_row['player_name'].iloc[0] if 'player_name' in target_row.columns else \"WR\"\n",
    "       \n",
    "        # Get QB info\n",
    "        qb_row = play_frames[play_frames['player_role'] == 'Passer']\n",
    "        qb_id = qb_row['nfl_id'].iloc[0] if not qb_row.empty else None\n",
    "        qb_name = qb_row['player_name'].iloc[0] if not qb_row.empty and 'player_name' in qb_row.columns else \"QB\"\n",
    "       \n",
    "        summary_row = self.summary_df[\n",
    "            (self.summary_df['game_id'] == game_id) &\n",
    "            (self.summary_df['play_id'] == play_id)\n",
    "        ]\n",
    "       \n",
    "        # Get Scores & Names\n",
    "        vis_score = 0.0\n",
    "        start_dist = 0.0\n",
    "        end_dist = 0.0\n",
    "        context_id = None\n",
    "        eraser_name = \"DEF\"\n",
    "        context_name = \"DEF\"\n",
    "       \n",
    "        if not summary_row.empty:\n",
    "            eraser_row = summary_row[summary_row['nfl_id'] == eraser_id]\n",
    "           \n",
    "            if not eraser_row.empty:\n",
    "                vis_score = eraser_row.iloc[0]['vis_score']\n",
    "                start_dist = eraser_row.iloc[0]['p_dist_at_throw']\n",
    "                end_dist = eraser_row.iloc[0].get('p_dist_at_arrival', start_dist - vis_score)\n",
    "               \n",
    "                if 'player_name' in eraser_row.columns:\n",
    "                    eraser_name = eraser_row.iloc[0]['player_name']\n",
    "           \n",
    "            # Get context defender (closest at throw)\n",
    "            context_idx = summary_row['p_dist_at_throw'].idxmin()\n",
    "            context_id = summary_row.loc[context_idx]['nfl_id']\n",
    "            if 'player_name' in summary_row.columns:\n",
    "                context_name = summary_row.loc[context_idx]['player_name']\n",
    "\n",
    "        # Get play metadata\n",
    "        meta = play_frames.iloc[0]\n",
    "        pass_result = meta.get('pass_result', 'Unknown')\n",
    "        yards_gained = meta.get('yards_gained', 0)\n",
    "        off_team = meta['possession_team']\n",
    "        def_team = meta['defensive_team']\n",
    "       \n",
    "        # Team colors\n",
    "        off_color = NFL_TEAM_COLORS.get(off_team, {'primary': '#c0392b'})['primary']\n",
    "        def_color = NFL_TEAM_COLORS.get(def_team, {'primary': '#2980b9'})['primary']\n",
    "        off_secondary = NFL_TEAM_COLORS.get(off_team, {'secondary': '#ffffff'}).get('secondary', '#ffffff')\n",
    "        def_secondary = NFL_TEAM_COLORS.get(def_team, {'secondary': '#ffffff'}).get('secondary', '#ffffff')\n",
    "       \n",
    "        # BALL TRAJECTORY LOGIC\n",
    "        unique_frames = sorted(play_frames['frame_id'].unique())\n",
    "        start_frame = play_frames['frame_id'].min()\n",
    "        end_frame = play_frames['frame_id'].max()\n",
    "       \n",
    "        # Identify the throw frame (first frame of post_throw phase)\n",
    "        if 'phase' in play_frames.columns:\n",
    "            post_throw_data = play_frames[play_frames['phase'] == 'post_throw']\n",
    "            if not post_throw_data.empty:\n",
    "                throw_frame = post_throw_data['frame_id'].min()\n",
    "            else:\n",
    "                throw_frame = start_frame\n",
    "        else:\n",
    "            throw_frame = start_frame\n",
    "       \n",
    "        # Pause at throw frame for emphasis (repeat frame 5 times at 10 fps ~0.5s pause)\n",
    "        frames_list = []\n",
    "        for f in unique_frames:\n",
    "            frames_list.append(f)\n",
    "            if f == throw_frame:\n",
    "                frames_list.extend([f] * 4)  # Add pause\n",
    "       \n",
    "        # Get post-throw frames for ball interpolation\n",
    "        post_throw_frames = sorted([f for f in unique_frames if f >= throw_frame])\n",
    "        post_throw_steps = len(post_throw_frames)\n",
    "       \n",
    "        # Ball landing position\n",
    "        bx_end, by_end = play_frames.iloc[0]['ball_land_x'], play_frames.iloc[0]['ball_land_y']\n",
    "       \n",
    "        # Get QB position at throw frame\n",
    "        passer_at_throw = play_frames[\n",
    "            (play_frames['frame_id'] == throw_frame) &\n",
    "            (play_frames['player_role'] == 'Passer')\n",
    "        ]\n",
    "        if not passer_at_throw.empty:\n",
    "            bx_throw, by_throw = passer_at_throw.iloc[0]['x'], passer_at_throw.iloc[0]['y']\n",
    "        else:\n",
    "            passer_early = play_frames[play_frames['player_role'] == 'Passer'].sort_values('frame_id')\n",
    "            if not passer_early.empty:\n",
    "                bx_throw, by_throw = passer_early.iloc[0]['x'], passer_early.iloc[0]['y']\n",
    "            else:\n",
    "                bx_throw, by_throw = bx_end - 15, 26.65\n",
    "       \n",
    "        # Interpolate ball trajectory ONLY for post-throw frames (slight curve for visual elite feel)\n",
    "        if post_throw_steps > 1:\n",
    "            t = np.linspace(0, 1, post_throw_steps)\n",
    "            ball_x_flight = bx_throw + (bx_end - bx_throw) * t\n",
    "            ball_y_flight = by_throw + (by_end - by_throw) * t + np.sin(t * np.pi) * 1.5  # Slight arc for aesthetics\n",
    "        else:\n",
    "            ball_x_flight = [bx_end]\n",
    "            ball_y_flight = [by_end]\n",
    "       \n",
    "        # Build ball position dictionary\n",
    "        ball_pos_dict = {}\n",
    "        for f in unique_frames:\n",
    "            if f < throw_frame:\n",
    "                passer_at_f = play_frames[\n",
    "                    (play_frames['frame_id'] == f) &\n",
    "                    (play_frames['player_role'] == 'Passer')\n",
    "                ]\n",
    "                if not passer_at_f.empty:\n",
    "                    ball_pos_dict[f] = (passer_at_f.iloc[0]['x'], passer_at_f.iloc[0]['y'])\n",
    "                else:\n",
    "                    ball_pos_dict[f] = (bx_throw, by_throw)\n",
    "            else:\n",
    "                idx = post_throw_frames.index(f)\n",
    "                ball_pos_dict[f] = (ball_x_flight[idx], ball_y_flight[idx])\n",
    "\n",
    "        # Cache last known position for each player to handle \"ghost\" players\n",
    "        # who disappear in post_throw frames (>8yds from catch point in output data)\n",
    "        all_player_ids = play_frames['nfl_id'].unique()\n",
    "        player_cache = {}\n",
    "       \n",
    "        # Pre-build cache by iterating through frames in order\n",
    "        player_positions_by_frame = {}\n",
    "        for f in unique_frames:\n",
    "            snap = play_frames[play_frames['frame_id'] == f]\n",
    "            frame_positions = {}\n",
    "           \n",
    "            for pid in all_player_ids:\n",
    "                player_row = snap[snap['nfl_id'] == pid]\n",
    "                if not player_row.empty:\n",
    "                   \n",
    "                    # Update cache with current position\n",
    "                    player_cache[pid] = {\n",
    "                        'x': player_row.iloc[0]['x'],\n",
    "                        'y': player_row.iloc[0]['y'],\n",
    "                        'role': player_row.iloc[0]['player_role'],\n",
    "                        's_derived': player_row.iloc[0].get('s_derived', 0),\n",
    "                        'phase': player_row.iloc[0]['phase']\n",
    "                    }\n",
    "               \n",
    "                # Use cached position (current or last known)\n",
    "                if pid in player_cache:\n",
    "                    frame_positions[pid] = player_cache[pid].copy()\n",
    "           \n",
    "            player_positions_by_frame[f] = frame_positions\n",
    "\n",
    "        # Setup Figure\n",
    "        fig, ax = plt.subplots(figsize=(14, 7))\n",
    "        self._draw_field(ax)\n",
    "       \n",
    "        # Context Box (Top Left) - Teams, Down & Distance, Coverage\n",
    "        yd_str = f\"{meta['yardline_side']} {int(meta['yardline_number'])}\"\n",
    "        cov_str = str(meta['team_coverage_type']).replace('_', ' ').title()\n",
    "        context_text = f\"{off_team} vs {def_team}\\n{int(meta['down'])} & {int(meta['yards_to_go'])} | {yd_str}\\n{cov_str}\"\n",
    "        at_context = AnchoredText(context_text, loc='upper left',\n",
    "                                  prop=dict(size=11, fontweight='bold', family='monospace'), frameon=True)\n",
    "        at_context.patch.set_boxstyle(\"round,pad=0.4\")\n",
    "        at_context.patch.set_facecolor('#1a1a2e')\n",
    "        at_context.patch.set_edgecolor('white')\n",
    "        at_context.patch.set_alpha(0.95)\n",
    "       \n",
    "        for txt in at_context.txt.get_children():\n",
    "            txt.set_color('white')\n",
    "       \n",
    "        ax.add_artist(at_context)\n",
    "       \n",
    "        # Metric Box (Top Center) - VIS Score with start/end\n",
    "        sign = \"+\" if vis_score > 0 else \"\"\n",
    "        metric_text = f\"START: {start_dist:.1f} yds\\nVIS: {sign}{vis_score:.1f} yds\\nEND: {end_dist:.1f} yds\"\n",
    "        at_metric = AnchoredText(metric_text, loc='upper center',\n",
    "                                 prop=dict(size=12, fontweight='bold', family='monospace'), frameon=True)\n",
    "       \n",
    "        at_metric.patch.set_boxstyle(\"round,pad=0.4\")\n",
    "        at_metric.patch.set_facecolor('#16213e')\n",
    "        at_metric.patch.set_edgecolor('#00ff88' if vis_score > 0 else '#ff4444')\n",
    "        at_metric.patch.set_linewidth(3)\n",
    "       \n",
    "        for txt in at_metric.txt.get_children():\n",
    "            txt.set_color('#00ff88' if vis_score > 0 else '#ff4444')\n",
    "       \n",
    "        ax.add_artist(at_metric)\n",
    "       \n",
    "        # Outcome Badge (Top Right area) - shows COMPLETE/INCOMPLETE with yards\n",
    "        outcome_color = '#27ae60' if pass_result == 'C' else '#c0392b'\n",
    "        outcome_text = f'COMPLETE  +{yards_gained} yds' if pass_result == 'C' else 'INCOMPLETE '\n",
    "        outcome_label = ax.text(118, 50, outcome_text, ha='right', va='top', fontsize=11,\n",
    "                                fontweight='bold', color='white',\n",
    "                                bbox=dict(facecolor=outcome_color, edgecolor='white',\n",
    "                                          pad=6, boxstyle='round,pad=0.4'))\n",
    "       \n",
    "        # Timer (Right side)\n",
    "        timer_text = ax.text(118, 42, '', ha='right', fontsize=13, fontweight='bold', color='white',\n",
    "                             bbox=dict(facecolor='#2c3e50', edgecolor='white', pad=4, boxstyle='round,pad=0.3'))\n",
    "       \n",
    "        # Phase Label (dynamic color based on phase)\n",
    "        phase_label = ax.text(118, 35, '', ha='right', fontsize=11, fontweight='bold', color='white',\n",
    "                              bbox=dict(facecolor='#3498db', edgecolor='white', pad=5, boxstyle='round,pad=0.3'))\n",
    "        phase_colors = {\n",
    "            'pre_throw': '#3498db',  # Blue\n",
    "            'post_throw': '#e74c3c',  # Red\n",
    "            'pre_snap': '#95a5a6',  # Gray\n",
    "            'post_snap': '#f39c12',  # Orange\n",
    "            'unknown': '#7f8c8d'\n",
    "        }\n",
    "       \n",
    "        # Speed indicator for eraser\n",
    "        speed_label = ax.text(118, 28, '', ha='right', fontsize=10, fontweight='bold', color='white',\n",
    "                              bbox=dict(facecolor='#8e44ad', edgecolor='white', pad=4, boxstyle='round,pad=0.3'))\n",
    "       \n",
    "        # Ball Landing Spot (X marker - visible entire animation, with glow)\n",
    "        ax.scatter([bx_end], [by_end], c='#ffff00', s=400, marker='X',\n",
    "                   edgecolors='#ff6600', linewidths=3, zorder=2, alpha=0.9)\n",
    "       \n",
    "        # Add subtle circle for target area\n",
    "        target_circle = Circle((bx_end, by_end), 1.5, color='#ffff00', alpha=0.2, zorder=1)\n",
    "        ax.add_patch(target_circle)\n",
    "        ax.text(bx_end, by_end - 2.5, 'TARGET', ha='center', va='top', fontsize=7,\n",
    "                fontweight='bold', color='#ffff00', alpha=0.9)\n",
    "       \n",
    "        # Other defenders (team color circles with shadow)\n",
    "        scat_def_others = ax.scatter([], [], c=def_color, s=200, marker='o',\n",
    "                                     edgecolors=def_secondary, linewidths=2, zorder=3, alpha=0.7)\n",
    "       \n",
    "        # Shadow for depth\n",
    "        scat_def_shadow = ax.scatter([], [], c='black', s=200, marker='o', alpha=0.2, zorder=2)\n",
    "       \n",
    "       \n",
    "        # Other offense (team color circles with shadow)\n",
    "        scat_off_others = ax.scatter([], [], c=off_color, s=200, marker='o',\n",
    "                                     edgecolors=off_secondary, linewidths=2, zorder=3, alpha=0.7)\n",
    "        scat_off_shadow = ax.scatter([], [], c='black', s=200, marker='o', alpha=0.2, zorder=2)\n",
    "       \n",
    "        # QB (diamond - distinctive, with team accent)\n",
    "        qb_color = off_color  # Use offense primary\n",
    "        scat_qb = ax.scatter([], [], c=qb_color, s=350, marker='D',\n",
    "                              edgecolors=off_secondary, linewidths=3, zorder=5)\n",
    "       \n",
    "        scat_qb_shadow = ax.scatter([], [], c='black', s=350, marker='D', alpha=0.2, zorder=4)\n",
    "        qb_label = ax.text(0, 0, '', ha='center', va='bottom', fontsize=8,\n",
    "                           fontweight='bold', color='white',\n",
    "                           bbox=dict(facecolor=qb_color, edgecolor='none', pad=2, alpha=0.9))\n",
    "       \n",
    "        # Target Receiver (star - stands out, with glow)\n",
    "        target_color = '#ff6b35'  # Keep orange for highlight\n",
    "        scat_target = ax.scatter([], [], c=target_color, s=450, marker='*',\n",
    "                                  edgecolors='white', linewidths=2, zorder=6)\n",
    "        scat_target_glow = ax.scatter([], [], c=target_color, s=600, marker='*', alpha=0.3, zorder=5)\n",
    "        target_label = ax.text(0, 0, '', ha='center', va='bottom', fontsize=8,\n",
    "                               fontweight='bold', color='white',\n",
    "                               bbox=dict(facecolor=target_color, edgecolor='none', pad=2, alpha=0.9))\n",
    "       \n",
    "        # Eraser (bright green, largest circle with dark edge)\n",
    "        eraser_color = '#00ff88'\n",
    "        scat_eraser = ax.scatter([], [], c=eraser_color, s=400, marker='o',\n",
    "                                  edgecolors='#004d40', linewidths=3, zorder=7)\n",
    "        scat_eraser_glow = ax.scatter([], [], c=eraser_color, s=550, marker='o', alpha=0.3, zorder=6)\n",
    "        eraser_label = ax.text(0, 0, '', ha='center', va='bottom', fontsize=8,\n",
    "                               fontweight='bold', color='black',\n",
    "                               bbox=dict(facecolor=eraser_color, edgecolor='none', pad=2, alpha=0.9))\n",
    "       \n",
    "        # Context Defender (triangle - closest at throw)\n",
    "        context_color = '#00bcd4'\n",
    "        scat_context = ax.scatter([], [], c=context_color, s=300, marker='^',\n",
    "                                   edgecolors='white', linewidths=2, zorder=5)\n",
    "        scat_context_shadow = ax.scatter([], [], c='black', s=300, marker='^', alpha=0.2, zorder=4)\n",
    "        context_label = ax.text(0, 0, '', ha='center', va='bottom', fontsize=8,\n",
    "                                fontweight='bold', color='white',\n",
    "                                bbox=dict(facecolor=context_color, edgecolor='none', pad=2, alpha=0.9))\n",
    "       \n",
    "        # Ball (football-shaped ellipse for more realistic NFL look)\n",
    "        ball_color = '#8B4513'\n",
    "        football_patch = Ellipse((0, 0), width=1.2, height=0.5, color=ball_color, edgecolor='white', linewidth=2, zorder=10)\n",
    "        ax.add_patch(football_patch)\n",
    "        # No rotation for simplicity, but could add based on velocity if desired\n",
    "       \n",
    "        # Void line (dashed line between eraser and target with distance)\n",
    "        line_void, = ax.plot([], [], color='#ffff00', linestyle='--', linewidth=2, alpha=0.8, zorder=4)\n",
    "        text_void = ax.text(0, 0, '', ha='center', va='center', fontsize=10, fontweight='bold',\n",
    "                            color='black', bbox=dict(facecolor='#ffff00', alpha=0.9, edgecolor='none', pad=3))\n",
    "       \n",
    "        trail_length = 30 # TODO: Adjust\n",
    "        target_history = []\n",
    "        eraser_history = []\n",
    "        context_history = []\n",
    "        qb_history = []\n",
    "        ball_history = []\n",
    "       \n",
    "        target_trail, = ax.plot([], [], color=target_color, lw=3, alpha=0.5, zorder=2, label='Target Path')\n",
    "        eraser_trail, = ax.plot([], [], color=eraser_color, lw=3, alpha=0.5, zorder=2, label='Eraser Path')\n",
    "        context_trail, = ax.plot([], [], color=context_color, lw=2, alpha=0.5, zorder=2, label='Context Path')\n",
    "        qb_trail, = ax.plot([], [], color=qb_color, lw=2, alpha=0.5, zorder=2, label='QB Path')\n",
    "        ball_trail, = ax.plot([], [], color=ball_color, lw=1.5, linestyle='-', alpha=0.4, zorder=1, label='Ball Path')\n",
    "       \n",
    "        legend_elements = [\n",
    "            plt.scatter([], [], c=eraser_color, s=150, marker='o', edgecolors='#004d40', linewidths=2, label='Eraser'),\n",
    "            plt.scatter([], [], c=target_color, s=150, marker='*', edgecolors='white', linewidths=2, label='Target WR'),\n",
    "            plt.scatter([], [], c=context_color, s=100, marker='^', edgecolors='white', linewidths=2, label='Nearest Def'),\n",
    "            plt.scatter([], [], c=qb_color, s=100, marker='D', edgecolors='white', linewidths=2, label='QB'),\n",
    "            patches.Ellipse((0,0), 1, 0.5, color=ball_color, label='Ball'),\n",
    "            plt.scatter([], [], c='#ffff00', s=100, marker='X', edgecolors='#ff6600', linewidths=2, label='Ball Target'),\n",
    "            plt.Line2D([], [], color='gray', lw=2, alpha=0.5, label='Player Trails')\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements, loc='lower right', fontsize=8,\n",
    "                  framealpha=0.95, facecolor='#1a1a2e', edgecolor='white', labelcolor='white',\n",
    "                  ncol=2)\n",
    "\n",
    "        # Update Loop\n",
    "        def update(frame_num):\n",
    "            # Use actual frame_id (since frames_list may have duplicates)\n",
    "            actual_frame = frame_num  # frames_list[frame_idx] but since ani uses range(len(frames_list))\n",
    "           \n",
    "            # Wait, ani = FuncAnimation(..., frames=frames_list, ...) so frame_num is the value from frames_list, i.e., the frame_id\n",
    "            f = frame_num  \n",
    "           \n",
    "            # Timer relative to throw frame\n",
    "            time_sec = (f - throw_frame) * 0.1\n",
    "            if f < throw_frame:\n",
    "                timer_text.set_text(f\"T {time_sec:.1f}s\")\n",
    "                phase_label.set_text(\" PRE THROW\")\n",
    "                phase_label.get_bbox_patch().set_facecolor('#3498db')\n",
    "            else:\n",
    "                timer_text.set_text(f\"T +{time_sec:.1f}s\")\n",
    "                phase_label.set_text(\" BALL IN AIR\")\n",
    "                phase_label.get_bbox_patch().set_facecolor('#e74c3c')\n",
    "           \n",
    "            # Ball position\n",
    "            if f in ball_pos_dict:\n",
    "                bx, by = ball_pos_dict[f]\n",
    "                football_patch.set_center((bx, by))\n",
    "                ball_history.append((bx, by))\n",
    "                if len(ball_history) > trail_length:\n",
    "                    ball_history.pop(0)\n",
    "                ball_trail.set_data(*zip(*ball_history))\n",
    "           \n",
    "            # Build position lists from cache\n",
    "            def_others_pos = []\n",
    "            off_others_pos = []\n",
    "            def_shadow_pos = []\n",
    "            off_shadow_pos = []\n",
    "           \n",
    "            excluded_ids = {eraser_id, target_id, context_id}\n",
    "            if qb_id:\n",
    "                excluded_ids.add(qb_id)\n",
    "           \n",
    "            frame_positions = player_positions_by_frame.get(f, {})\n",
    "            for pid, pos in frame_positions.items():\n",
    "                if pid in excluded_ids:\n",
    "                    continue\n",
    "                role = pos.get('role', '')\n",
    "                px, py = pos['x'], pos['y']\n",
    "                if role in ['Coverage Defender', 'Pass Rusher', 'Defender']:\n",
    "                    def_others_pos.append([px, py])\n",
    "                    def_shadow_pos.append([px + 0.2, py - 0.2])  # Offset shadow\n",
    "                elif role not in ['Passer']:\n",
    "                    off_others_pos.append([px, py])\n",
    "                    off_shadow_pos.append([px + 0.2, py - 0.2])\n",
    "           \n",
    "            scat_def_others.set_offsets(np.array(def_others_pos) if def_others_pos else np.empty((0, 2)))\n",
    "            scat_def_shadow.set_offsets(np.array(def_shadow_pos) if def_shadow_pos else np.empty((0, 2)))\n",
    "            scat_off_others.set_offsets(np.array(off_others_pos) if off_others_pos else np.empty((0, 2)))\n",
    "            scat_off_shadow.set_offsets(np.array(off_shadow_pos) if off_shadow_pos else np.empty((0, 2)))\n",
    "           \n",
    "            # QB (from cache)\n",
    "            if qb_id and qb_id in frame_positions:\n",
    "                qb_pos = frame_positions[qb_id]\n",
    "                qx, qy = qb_pos['x'], qb_pos['y']\n",
    "                scat_qb.set_offsets([[qx, qy]])\n",
    "                scat_qb_shadow.set_offsets([[qx + 0.2, qy - 0.2]])\n",
    "                qb_label.set_position((qx, qy + 2.5))\n",
    "                q_name = str(qb_name).split()[-1][:8] if qb_name else \"QB\"\n",
    "                qb_label.set_text(q_name)\n",
    "                qb_history.append((qx, qy))\n",
    "                if len(qb_history) > trail_length:\n",
    "                    qb_history.pop(0)\n",
    "                qb_trail.set_data(*zip(*qb_history))\n",
    "            else:\n",
    "                scat_qb.set_offsets(np.empty((0, 2)))\n",
    "                scat_qb_shadow.set_offsets(np.empty((0, 2)))\n",
    "           \n",
    "            # Target (from cache)\n",
    "            if target_id in frame_positions:\n",
    "                target_pos = frame_positions[target_id]\n",
    "                tx, ty = target_pos['x'], target_pos['y']\n",
    "                scat_target.set_offsets([[tx, ty]])\n",
    "                scat_target_glow.set_offsets([[tx, ty]])\n",
    "                target_label.set_position((tx, ty + 2.5))\n",
    "                t_name = str(target_name).split()[-1][:8] if target_name else \"WR\"\n",
    "                target_label.set_text(t_name)\n",
    "                target_history.append((tx, ty))\n",
    "                if len(target_history) > trail_length:\n",
    "                    target_history.pop(0)\n",
    "                target_trail.set_data(*zip(*target_history))\n",
    "           \n",
    "            # Context defender (from cache, with name label)\n",
    "            if context_id and context_id in frame_positions and context_id != eraser_id:\n",
    "                context_pos = frame_positions[context_id]\n",
    "                cx, cy = context_pos['x'], context_pos['y']\n",
    "                scat_context.set_offsets([[cx, cy]])\n",
    "                scat_context_shadow.set_offsets([[cx + 0.2, cy - 0.2]])\n",
    "                context_label.set_position((cx, cy + 2.5))\n",
    "                c_name = str(context_name).split()[-1][:8] if context_name else \"DEF\"\n",
    "                context_label.set_text(c_name)\n",
    "                context_history.append((cx, cy))\n",
    "                if len(context_history) > trail_length:\n",
    "                    context_history.pop(0)\n",
    "                context_trail.set_data(*zip(*context_history))\n",
    "            else:\n",
    "                scat_context.set_offsets(np.empty((0, 2)))\n",
    "                scat_context_shadow.set_offsets(np.empty((0, 2)))\n",
    "                context_label.set_text('')\n",
    "               \n",
    "            # Eraser (from cache)\n",
    "            if eraser_id in frame_positions:\n",
    "                eraser_pos = frame_positions[eraser_id]\n",
    "                ex, ey = eraser_pos['x'], eraser_pos['y']\n",
    "                scat_eraser.set_offsets([[ex, ey]])\n",
    "                scat_eraser_glow.set_offsets([[ex, ey]])\n",
    "                eraser_label.set_position((ex, ey + 2.5))\n",
    "                e_name = str(eraser_name).split()[-1][:8] if eraser_name else \"DEF\"\n",
    "                eraser_label.set_text(e_name)\n",
    "               \n",
    "                # Speed display\n",
    "                speed_yps = eraser_pos.get('s_derived', 0)\n",
    "                speed_mph = speed_yps * 2.045 if pd.notna(speed_yps) else 0\n",
    "                speed_label.set_text(f\" {speed_mph:.1f} mph\")\n",
    "               \n",
    "                # Void line\n",
    "                if target_id in frame_positions:\n",
    "                    line_void.set_data([ex, tx], [ey, ty])\n",
    "                    dist = np.sqrt((ex-tx)**2 + (ey-ty)**2)\n",
    "                    text_void.set_position(((ex+tx)/2, (ey+ty)/2))\n",
    "                    text_void.set_text(f\"{dist:.1f} yds\")\n",
    "                eraser_history.append((ex, ey))\n",
    "                if len(eraser_history) > trail_length:\n",
    "                    eraser_history.pop(0)\n",
    "                eraser_trail.set_data(*zip(*eraser_history))\n",
    "           \n",
    "            return (scat_def_others, scat_def_shadow, scat_off_others, scat_off_shadow, scat_target, scat_target_glow,\n",
    "                    scat_eraser, scat_eraser_glow, scat_context, scat_context_shadow, scat_qb, scat_qb_shadow,\n",
    "                    football_patch, line_void, text_void, timer_text, phase_label, speed_label,\n",
    "                    target_label, eraser_label, qb_label, context_label,\n",
    "                    target_trail, eraser_trail, context_trail, qb_trail, ball_trail)\n",
    "\n",
    "        ani = animation.FuncAnimation(fig, update, frames=frames_list, interval=100, blit=True)\n",
    "       \n",
    "        save_path = os.path.join(self.output_dir, filename)\n",
    "        ani.save(save_path, writer='pillow', fps=10)\n",
    "        plt.close()\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, summary_path, frames_path):\n",
    "        self.summary_path = summary_path\n",
    "        self.frames_path = frames_path\n",
    "        self.summary_df = None\n",
    "        self.frames_df = None\n",
    "\n",
    "    def load_data(self):\n",
    "        print(f\"   [Loader] Loading Summary Data...\")\n",
    "        self.summary_df = pd.read_csv(self.summary_path)\n",
    "        self.frames_df = pd.read_csv(self.frames_path)\n",
    "        \n",
    "        return self.summary_df, self.frames_df\n",
    "\n",
    "class StoryDataEngine:\n",
    "    def __init__(self, summary_df: str, frames_df: str, seed=42):\n",
    "        self.summary_df = summary_df\n",
    "        self.frames_df = frames_df\n",
    "        self.seed = seed\n",
    "        \n",
    "    def cast_archetypes(self):\n",
    "        \"\"\"\n",
    "        UPDATED: Selects candidates based on the new VIS thresholds.\n",
    "        Returns keys: 'The Eraser', 'The Rally', 'The Blanket', 'The Liability'\n",
    "        \"\"\"\n",
    "        df = self.summary_df\n",
    "        \n",
    "        # THE ERASER (Elite Recovery)\n",
    "        # Criteria: Deep start (>10), Massive VIS (>3)\n",
    "        eraser_pool = df[\n",
    "            (df['p_dist_at_throw'] > 10) & \n",
    "            (df['vis_score'] > 3.0) & \n",
    "            (df['dist_at_arrival'] < 4) \n",
    "        ]\n",
    "        eraser = self._select_candidate(eraser_pool, sort_col='vis_score', ascending=False)\n",
    "        \n",
    "        # THE RALLY (Standard Zone / The Blob)\n",
    "        # Criteria: Deep start (>8), Moderate VIS (0.5 to 2.0)\n",
    "        rally_pool = df[\n",
    "            (df['p_dist_at_throw'] > 8) & \n",
    "            (df['vis_score'].between(0.5, 2.0)) &\n",
    "            (df['dist_at_arrival'] < 8)\n",
    "        ]\n",
    "        rally = self._select_candidate(rally_pool, sort_col='vis_score', ascending=False)\n",
    "        \n",
    "        # THE BLANKET (Maintenance)\n",
    "        # Criteria: Tight start (<3), VIS near zero (-0.5 to 0.5)\n",
    "        blanket_pool = df[\n",
    "            (df['p_dist_at_throw'] < 3) & \n",
    "            (df['vis_score'].between(-0.5, 0.5)) &\n",
    "            (df['dist_at_arrival'] < 3)\n",
    "        ]\n",
    "        blanket = self._select_candidate(blanket_pool, sort_col='dist_at_arrival', ascending=True)\n",
    "        \n",
    "        # THE LIABILITY (True Failure)\n",
    "        # Criteria: Any start, Negative VIS (Lost Ground)\n",
    "        liability_pool = df[\n",
    "            (df['vis_score'] < -1.0) & \n",
    "            (df['dist_at_arrival'] > 2.0)\n",
    "        ]\n",
    "        liability = self._select_candidate(liability_pool, sort_col='vis_score', ascending=True)\n",
    "\n",
    "        return {\n",
    "            'The Eraser': self._extract_meta(eraser, \"The Eraser (Elite)\"),\n",
    "            'The Rally': self._extract_meta(rally, \"The Rally (Zone)\"),\n",
    "            'The Blanket': self._extract_meta(blanket, \"The Blanket (Lockdown)\"),\n",
    "            'The Liability': self._extract_meta(liability, \"The Liability (Lost Gap)\")\n",
    "        }\n",
    "\n",
    "    def _select_candidate(self, df, sort_col, ascending, top_n=1):\n",
    "        if df.empty: return pd.DataFrame()\n",
    "        sorted_df = df.sort_values(sort_col, ascending=ascending)\n",
    "        df = sorted_df.head(top_n)\n",
    "        print(df.get(\"player_name\", \"Unknown Player\"))\n",
    "        return df\n",
    "\n",
    "    def _extract_meta(self, row, label):\n",
    "        if row.empty: return None\n",
    "        return {\n",
    "            'game_id': int(row.iloc[0]['game_id']),\n",
    "            'play_id': int(row.iloc[0]['play_id']),\n",
    "            'nfl_id': float(row.iloc[0]['nfl_id']),\n",
    "            'player_name': str(row.iloc[0].get('player_name', 'Unknown')),\n",
    "            'vis_score': float(row.iloc[0]['vis_score']),\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "    def get_position_contrast(self, position='FS', min_snaps=15):\n",
    "        \"\"\"\n",
    "        Apple-to-apple comparison: Top vs Bottom player for the same position.\n",
    "        \n",
    "        1. Ranks players by avg CEOE within the position\n",
    "        2. Top player  their best VIS play (biggest close)\n",
    "        3. Bottom player  their worst VIS play (biggest loss)\n",
    "        \"\"\"\n",
    "        df = self.summary_df\n",
    "        pos_df = df[df['player_position'] == position].copy()\n",
    "        \n",
    "        if pos_df.empty:\n",
    "            print(f\"No plays found for position: {position}\")\n",
    "            return {'top': None, 'bottom': None}\n",
    "        \n",
    "        # Rank players by average CEOE\n",
    "        player_ranks = pos_df.groupby('nfl_id').agg(\n",
    "            player_name=('player_name', 'first'),\n",
    "            avg_ceoe=('ceoe_score', 'mean'),\n",
    "            snaps=('play_id', 'count')\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Filter for minimum snaps\n",
    "        qualified = player_ranks[player_ranks['snaps'] >= min_snaps]\n",
    "        \n",
    "        if len(qualified) < 2:\n",
    "            print(f\"Not enough qualified players for {position} (need 2, have {len(qualified)})\")\n",
    "            return {'top': None, 'bottom': None}\n",
    "        \n",
    "        top_player = qualified.nlargest(1, 'avg_ceoe').iloc[0]\n",
    "        bottom_player = qualified.nsmallest(1, 'avg_ceoe').iloc[0]\n",
    "        \n",
    "        top_id = top_player['nfl_id']\n",
    "        bottom_id = bottom_player['nfl_id']\n",
    "\n",
    "        # Find their best/worst plays\n",
    "        top_plays = pos_df[pos_df['nfl_id'] == top_id]\n",
    "        bottom_plays = pos_df[pos_df['nfl_id'] == bottom_id]\n",
    "        \n",
    "        # Top player's BEST play (highest VIS = biggest close)\n",
    "        top_best = top_plays.nlargest(1, 'vis_score')\n",
    "        \n",
    "        # Bottom player's WORST play (lowest VIS = biggest loss)\n",
    "        bottom_worst = bottom_plays.nsmallest(1, 'vis_score')\n",
    "        \n",
    "        print(f\"Top's best play: VIS = {top_best.iloc[0]['vis_score']:.2f}\")\n",
    "        print(f\"Bottom's worst play: VIS = {bottom_worst.iloc[0]['vis_score']:.2f}\")\n",
    "        \n",
    "        return {\n",
    "            'top': self._extract_meta(top_best, f\"Top {position} Eraser\"),\n",
    "            'bottom': self._extract_meta(bottom_worst, f\"Bottom {position} Eraser\")\n",
    "        }\n",
    "\n",
    "    def get_archetype_contrast(self, min_snaps=15):\n",
    "        \"\"\"\n",
    "        Role contrast: Top FS Eraser vs Top CB Lockdown.\n",
    "        \n",
    "        Shows WHY different positions should be graded differently:\n",
    "        - FS: Closes big gaps (high VIS)\n",
    "        - CB: Stays tight (low dist_at_arrival)\n",
    "        \"\"\"\n",
    "        df = self.summary_df\n",
    "        \n",
    "        fs_df = df[df['player_position'] == 'FS'].copy()\n",
    "        fs_ranks = fs_df.groupby('nfl_id').agg(\n",
    "            player_name=('player_name', 'first'),\n",
    "            avg_ceoe=('ceoe_score', 'mean'),\n",
    "            snaps=('play_id', 'count')\n",
    "        ).reset_index()\n",
    "        fs_qualified = fs_ranks[fs_ranks['snaps'] >= min_snaps]\n",
    "        \n",
    "        if fs_qualified.empty:\n",
    "            print(\"No qualified FS players\")\n",
    "            return {'eraser': None, 'lockdown': None}\n",
    "        \n",
    "        top_fs = fs_qualified.nlargest(1, 'avg_ceoe').iloc[0]\n",
    "        top_fs_plays = fs_df[fs_df['nfl_id'] == top_fs['nfl_id']]\n",
    "        eraser_play = top_fs_plays.nlargest(1, 'vis_score')\n",
    "        \n",
    "        cb_df = df[df['player_position'] == 'CB'].copy()\n",
    "        cb_ranks = cb_df.groupby('nfl_id').agg(\n",
    "            player_name=('player_name', 'first'),\n",
    "            avg_arrival=('dist_at_arrival', 'mean'),\n",
    "            snaps=('play_id', 'count')\n",
    "        ).reset_index()\n",
    "        cb_qualified = cb_ranks[cb_ranks['snaps'] >= min_snaps]\n",
    "        \n",
    "        if cb_qualified.empty:\n",
    "            print(\"No qualified CB players\")\n",
    "            return {'eraser': None, 'lockdown': None}\n",
    "        \n",
    "        top_cb = cb_qualified.nsmallest(1, 'avg_arrival').iloc[0]  \n",
    "        top_cb_plays = cb_df[cb_df['nfl_id'] == top_cb['nfl_id']]\n",
    "        lockdown_play = top_cb_plays.nsmallest(1, 'dist_at_arrival') \n",
    "\n",
    "        return {\n",
    "            'eraser': self._extract_meta(eraser_play, f\"Top FS Eraser: {top_fs['player_name']}\"),\n",
    "            'lockdown': self._extract_meta(lockdown_play, f\"Top CB Lockdown: {top_cb['player_name']}\")\n",
    "        }\n",
    "\n",
    "    def get_play_frames(self, play_meta):\n",
    "        if not play_meta: return pd.DataFrame()\n",
    "        \n",
    "        return self.frames_df[\n",
    "            (self.frames_df['game_id'] == play_meta['game_id']) & \n",
    "            (self.frames_df['play_id'] == play_meta['play_id'])\n",
    "        ].copy()\n",
    "\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\")\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "\n",
    "class StoryVisualEngine:\n",
    "    def __init__(self, summary_df, frames_df, output_dir):\n",
    "        self.summary_df = summary_df\n",
    "        self.frames_df = frames_df\n",
    "        \n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        self.quad_colors = {\n",
    "            'The Eraser': '#1e8449',      # Dark Green (Elite)\n",
    "            'The Rally': '#82e0aa',       # Light Green (Active Zone)\n",
    "            'The Blanket': '#95a5a6',     # Grey (Maintenance)\n",
    "            'The Liability': '#e74c3c',   # Red (Lost Ground)\n",
    "            'Neutral': '#ecf0f1'\n",
    "        }\n",
    "\n",
    "    def plot_eraser_landscape(self, cast_dict):\n",
    "        df = self.summary_df.copy()\n",
    "        \n",
    "        conditions = [\n",
    "            (df['vis_score'] > 2.0),                     # The Eraser\n",
    "            (df['vis_score'].between(0.5, 2.0)),         # The Rally\n",
    "            (df['vis_score'].between(-0.5, 0.5)),        # The Blanket\n",
    "            (df['vis_score'] < -0.5)                     # The Liability\n",
    "        ]\n",
    "\n",
    "        choices = ['The Eraser', 'The Rally', 'The Blanket', 'The Liability']\n",
    "        df['quadrant_plot'] = np.select(conditions, choices, default='Neutral')\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        \n",
    "        # Main Scatter\n",
    "        hue_order = ['The Eraser', 'The Rally', 'The Blanket', 'The Liability']\n",
    "        \n",
    "        sns.scatterplot(\n",
    "            data=df, x='p_dist_at_throw', y='dist_at_arrival',\n",
    "            hue='quadrant_plot', palette=self.quad_colors,\n",
    "            hue_order=hue_order,\n",
    "            alpha=0.6, s=40, linewidth=0, ax=ax\n",
    "        )\n",
    "        \n",
    "        # Reference Lines\n",
    "        ax.plot([0, 25], [0, 25], 'k--', lw=2, alpha=0.3, label='Break Even (VIS=0)')\n",
    "        \n",
    "        # Add the Threshold visual aids (The \"Buffer Zone\")\n",
    "        x_vals = np.array([0, 25])\n",
    "        ax.plot(x_vals, x_vals - 0.5, color='gray', linestyle=':', lw=1, alpha=0.5)\n",
    "        ax.plot(x_vals, x_vals + 0.5, color='gray', linestyle=':', lw=1, alpha=0.5)\n",
    "\n",
    "        # Annotations\n",
    "        for role, play_meta in cast_dict.items():\n",
    "            if play_meta is None: continue\n",
    "            \n",
    "            # Find coordinates in summary\n",
    "            row = df[(df['game_id'] == play_meta['game_id']) & \n",
    "                     (df['play_id'] == play_meta['play_id']) & \n",
    "                     (df['nfl_id'] == play_meta['nfl_id'])]\n",
    "            \n",
    "            if not row.empty:\n",
    "                sx = row.iloc[0]['p_dist_at_throw']\n",
    "                sy = row.iloc[0]['dist_at_arrival']\n",
    "                \n",
    "                # Annotate\n",
    "                ax.annotate(play_meta['label'], \n",
    "                            (sx, sy), xytext=(sx+2, sy+2),\n",
    "                            arrowprops=dict(facecolor='black', shrink=0.05, width=1, headwidth=8),\n",
    "                            fontsize=11, fontweight='bold', \n",
    "                            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", lw=1))\n",
    "\n",
    "        ax.set_title('The Eraser Landscape: Recovery vs. Result', fontsize=18, fontweight='bold', pad=20)\n",
    "        ax.set_xlabel('Start Distance (The Mess)', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('End Distance (The Finish)', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlim(0, 20)\n",
    "        ax.set_ylim(0, 20)\n",
    "        ax.legend(title='Defensive Role', loc='upper right')\n",
    "        \n",
    "        output_path = os.path.join(self.output_dir, 'V1_Eraser_Landscape.png')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_race_charts(self, cast_dict):        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12)) \n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        quad_order = ['The Eraser', 'The Rally', 'The Blanket', 'The Liability']\n",
    "\n",
    "        # Legend Elements\n",
    "        legend_elements = [\n",
    "            plt.Line2D([0], [0], color='grey', lw=3, label='Defender Path'),\n",
    "            plt.Line2D([0], [0], color='gray', linestyle=':', label='Closed (1.5y)'),\n",
    "            plt.Line2D([0], [0], marker='o', color='grey', label='Throw', markersize=10, linestyle='None'),\n",
    "            plt.Line2D([0], [0], marker='X', color='grey', label='Arrival', markersize=10, linestyle='None')\n",
    "        ]\n",
    "\n",
    "        for i, quad_name in enumerate(quad_order):\n",
    "            ax = axes[i]\n",
    "            play_meta = cast_dict.get(quad_name)\n",
    "            \n",
    "            # Default handling if play_meta is missing\n",
    "            if play_meta is None:\n",
    "                ax.text(0.5, 0.5, f\"No Data for {quad_name}\", ha='center', va='center')\n",
    "                continue\n",
    "\n",
    "            color = self.quad_colors.get(quad_name, 'grey')\n",
    "\n",
    "            # Title with VIS Score\n",
    "            vis_val = play_meta['vis_score']\n",
    "            sign = \"+\" if vis_val > 0 else \"\"\n",
    "            ax.set_title(f\"{quad_name}\\n(VIS: {sign}{vis_val:.1f} yds)\", \n",
    "                         fontsize=16, fontweight='bold', color=color)\n",
    "\n",
    "            # Get Tracking Data\n",
    "            play_df = self.frames_df[\n",
    "                (self.frames_df['game_id'] == play_meta['game_id']) & \n",
    "                (self.frames_df['play_id'] == play_meta['play_id'])\n",
    "            ]\n",
    "            \n",
    "            def_track = play_df[play_df['nfl_id'] == play_meta['nfl_id']].sort_values('frame_id')\n",
    "            target_track = play_df[play_df['player_role'] == 'Targeted Receiver'].sort_values('frame_id')\n",
    "\n",
    "            if def_track.empty or target_track.empty:\n",
    "                continue\n",
    "                    \n",
    "            merged = pd.merge(def_track, target_track, on='frame_id', suffixes=('_d', '_t'))\n",
    "            merged['dist'] = np.sqrt((merged['x_d'] - merged['x_t'])**2 + (merged['y_d'] - merged['y_t'])**2)\n",
    "            merged['time_sec'] = (merged['frame_id'] - merged['frame_id'].min()) * 0.1\n",
    "\n",
    "            # Spline Smoothing\n",
    "            time_arr = merged['time_sec'].values\n",
    "            dist_arr = merged['dist'].values\n",
    "            if len(time_arr) >= 4:\n",
    "                try:\n",
    "                    spline = UnivariateSpline(time_arr, dist_arr, s=50)\n",
    "                    smooth_dist = spline(time_arr)\n",
    "                except:\n",
    "                    smooth_dist = dist_arr\n",
    "            else:\n",
    "                smooth_dist = dist_arr\n",
    "            merged['smooth_dist'] = smooth_dist\n",
    "            \n",
    "            max_dist = merged['smooth_dist'].max()\n",
    "            max_time = merged['time_sec'].max()\n",
    "\n",
    "            # PLOT LOGIC\n",
    "            ax.axhspan(0, 1.5, color='#d5d5d5', alpha=0.4, zorder=0)\n",
    "            ax.text(max_time * 0.95, 0.75, 'CONTESTED\\nZONE', ha='right', va='center',\n",
    "                   fontsize=8, color='#666666', style='italic', alpha=0.8)\n",
    "            \n",
    "            ax.plot(merged['time_sec'], merged['smooth_dist'], lw=4, color=color, alpha=0.9)\n",
    "            ax.fill_between(merged['time_sec'], merged['smooth_dist'], 0, color=color, alpha=0.1)\n",
    "\n",
    "            start = merged.iloc[0]\n",
    "            end = merged.iloc[-1]\n",
    "            \n",
    "            ax.scatter(start['time_sec'], start['smooth_dist'], color=color, s=150, marker='o', zorder=5, edgecolors='white', lw=2)\n",
    "            ax.annotate('THROW', (start['time_sec'], start['smooth_dist']), xytext=(5, 5), textcoords='offset points', fontsize=9, fontweight='bold', color=color)\n",
    "\n",
    "            ax.scatter(end['time_sec'], end['smooth_dist'], color=color, s=150, marker='X', zorder=5, edgecolors='white', lw=2)\n",
    "            ax.annotate('ARRIVAL', (end['time_sec'], end['smooth_dist']), xytext=(5, 5), textcoords='offset points', fontsize=9, fontweight='bold', color=color)\n",
    "\n",
    "            # Flight Time (Save for footer)\n",
    "            play_meta['flight_time'] = end['time_sec'] - start['time_sec']\n",
    "\n",
    "            # Formatting\n",
    "            ax.axhline(1.5, color='gray', linestyle=':', lw=2)\n",
    "            ax.axhline(0, color='black', lw=1)\n",
    "            ax.set_ylim(-0.5, max(max_dist * 1.15, 5)) \n",
    "            ax.set_xlim(0, max_time + 0.3)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            if i in [2, 3]: ax.set_xlabel('Seconds After Throw', fontsize=12, fontweight='bold')\n",
    "            if i in [0, 2]: ax.set_ylabel('Separation (Yards)', fontsize=12, fontweight='bold')\n",
    "\n",
    "        # Global Legend\n",
    "        fig.legend(handles=legend_elements, loc='lower center', ncol=4, bbox_to_anchor=(0.5, -0.02), frameon=False, fontsize=11)\n",
    "        \n",
    "        # Flight Time annotation\n",
    "        flight_parts = []\n",
    "        for quad_name in quad_order:\n",
    "            meta = cast_dict.get(quad_name)\n",
    "            if meta and 'flight_time' in meta:\n",
    "                flight_parts.append(f\"{quad_name}: {meta['flight_time']:.1f}s\")\n",
    "        \n",
    "        if flight_parts:\n",
    "            flight_text = \"Ball Flight Time  \" + \" | \".join(flight_parts)\n",
    "            fig.text(0.5, -0.06, flight_text, ha='center', va='top', fontsize=10, color='#555555', style='italic')\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0.02, 1, 1])\n",
    "        output_path = os.path.join(self.output_dir, 'V2_Race_Charts.png')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_coverage_heatmap(self):\n",
    "        \"\"\"\n",
    "        Average VIS by Route Depth vs Coverage Type.\n",
    "        \"\"\"\n",
    "        df = self.summary_df.copy()\n",
    "\n",
    "        # Binning\n",
    "        bins = [-5, 5, 10, 15, 25, 100]\n",
    "        labels = ['Behind LOS', 'Short (0-5)', 'Medium (5-10)', 'Int (10-15)', 'Deep (15+)']\n",
    "        df['depth_band'] = pd.cut(df['pass_length'], bins=bins, labels=labels)\n",
    "\n",
    "        # Filter\n",
    "        main_coverages = ['COVER_1', 'COVER_2_MAN', 'COVER_2_ZONE', 'COVER_3_ZONE', 'COVER_4_ZONE', 'COVER_6_ZONE']\n",
    "        df = df[df['team_coverage_type'].isin(main_coverages)]\n",
    "\n",
    "        # Pivot\n",
    "        grouped = df.groupby(['team_coverage_type', 'depth_band'], observed=False)\n",
    "        heatmap_data = grouped['vis_score'].mean()\n",
    "        counts = grouped.size()\n",
    "        heatmap_data = heatmap_data.where(counts >= 10).unstack()\n",
    "\n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        cmap = sns.diverging_palette(10, 130, as_cmap=True) # Red-Green\n",
    "        \n",
    "        sns.heatmap(\n",
    "            heatmap_data, annot=True, fmt=\".1f\", cmap=cmap,\n",
    "            center=0, vmin=-2, vmax=4, linewidths=.5,\n",
    "            cbar_kws={'label': 'Average VIS (Yards Erased)'}, ax=ax\n",
    "        )\n",
    "\n",
    "        ax.set_title('Where Defenses Erase Space: Scheme vs. Depth', fontsize=16, fontweight='bold', pad=20)\n",
    "        ax.set_xlabel('Target Depth (Air Yards)', fontsize=14)\n",
    "        ax.set_ylabel('Coverage Scheme', fontsize=14)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        output_path = os.path.join(self.output_dir, 'V3_Coverage_Heatmap.png')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_effort_impact_chart(self):\n",
    "        \"\"\"\n",
    "        Slope chart showing Effort  Outcome (EPA/YAC saved).\n",
    "        Ties the story together: Context  Effort  Result.\n",
    "        \"\"\"\n",
    "        df = self.summary_df.copy()\n",
    "        \n",
    "        completed = df[df['pass_result'] == 'C'].copy()\n",
    "\n",
    "        # Derive YAC\n",
    "        completed['yac'] = completed['yards_gained'] - completed['pass_length']\n",
    "        \n",
    "        # Create Start Distance bands\n",
    "        dist_bins = [0, 3, 6, 10, 100]\n",
    "        dist_labels = ['Tight\\n(0-3 yds)', 'Medium\\n(3-6 yds)', 'High Void\\n(6-10 yds)', 'Exempt\\n(10+ yds)']\n",
    "        completed['start_band'] = pd.cut(completed['p_dist_at_throw'], bins=dist_bins, labels=dist_labels)\n",
    "        \n",
    "        # Calculate VIS quartiles WITHIN each start band\n",
    "        def get_quartile_label(group):\n",
    "            q25 = group['vis_score'].quantile(0.25)\n",
    "            q75 = group['vis_score'].quantile(0.75)\n",
    "            conditions = [\n",
    "                group['vis_score'] <= q25,\n",
    "                group['vis_score'] >= q75\n",
    "            ]\n",
    "            choices = ['Low Effort', 'High Effort']\n",
    "            group['effort_bucket'] = np.select(conditions, choices, default='Middle')\n",
    "            return group\n",
    "        \n",
    "        completed = completed.groupby('start_band', group_keys=False, observed=False).apply(get_quartile_label)\n",
    "        \n",
    "        extremes = completed[completed['effort_bucket'].isin(['Low Effort', 'High Effort'])]\n",
    "        impact_data = extremes.groupby(['start_band', 'effort_bucket'], observed=False).agg(\n",
    "            avg_epa=('expected_points_added', 'mean'),\n",
    "            avg_yac=('yac', 'mean'),\n",
    "            count=('play_id', 'count')\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Create the figure with two subplots (EPA and YAC)\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        \n",
    "        bands = ['Tight\\n(0-3 yds)', 'Medium\\n(3-6 yds)', 'High Void\\n(6-10 yds)', 'Exempt\\n(10+ yds)']\n",
    "        x_positions = np.arange(len(bands))\n",
    "        \n",
    "        for ax_idx, (metric, title, ylabel) in enumerate([\n",
    "            ('avg_epa', 'EPA Impact: Effort Saves Points', 'Expected Points Added'),\n",
    "            ('avg_yac', 'YAC Impact: Effort Limits Damage', 'Yards After Catch')\n",
    "        ]):\n",
    "            ax = axes[ax_idx]\n",
    "            \n",
    "            # Get data for each effort level\n",
    "            low_effort = []\n",
    "            high_effort = []\n",
    "            savings = []\n",
    "            \n",
    "            for band in bands:\n",
    "                low_row = impact_data[(impact_data['start_band'] == band) & (impact_data['effort_bucket'] == 'Low Effort')]\n",
    "                high_row = impact_data[(impact_data['start_band'] == band) & (impact_data['effort_bucket'] == 'High Effort')]\n",
    "                \n",
    "                low_val = low_row[metric].values[0] if not low_row.empty else np.nan\n",
    "                high_val = high_row[metric].values[0] if not high_row.empty else np.nan\n",
    "                \n",
    "                low_effort.append(low_val)\n",
    "                high_effort.append(high_val)\n",
    "                savings.append(low_val - high_val if pd.notna(low_val) and pd.notna(high_val) else np.nan)\n",
    "            \n",
    "            # Plot bars - calmer colors\n",
    "            bar_width = 0.35\n",
    "            bars_low = ax.bar(x_positions - bar_width/2, low_effort, bar_width, \n",
    "                             label='Low Effort (Q1)', color='#d98880', alpha=0.85, edgecolor='white', linewidth=1.5)\n",
    "            bars_high = ax.bar(x_positions + bar_width/2, high_effort, bar_width, \n",
    "                              label='High Effort (Q4)', color='#7dcea0', alpha=0.85, edgecolor='white', linewidth=1.5)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar in bars_low:\n",
    "                height = bar.get_height()\n",
    "                if pd.notna(height):\n",
    "                    ax.annotate(f'{height:.2f}',\n",
    "                               xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                               xytext=(0, 3), textcoords=\"offset points\",\n",
    "                               ha='center', va='bottom', fontsize=9, fontweight='bold', color='#943126')\n",
    "            \n",
    "            for bar in bars_high:\n",
    "                height = bar.get_height()\n",
    "                if pd.notna(height):\n",
    "                    ax.annotate(f'{height:.2f}',\n",
    "                               xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                               xytext=(0, 3), textcoords=\"offset points\",\n",
    "                               ha='center', va='bottom', fontsize=9, fontweight='bold', color='#1e8449')\n",
    "            \n",
    "            # Add savings labels (no arrows, positioned at bottom of chart to avoid overlap)\n",
    "            max_height = max([v for v in low_effort + high_effort if pd.notna(v)]) if any(pd.notna(v) for v in low_effort + high_effort) else 2\n",
    "            \n",
    "            for i, saved in enumerate(savings):\n",
    "                if pd.notna(saved) and saved > 0:\n",
    "                    unit = 'EPA' if metric == 'avg_epa' else 'YAC'\n",
    "                    # Position label below x-axis to avoid overlap with title\n",
    "                    ax.text(x_positions[i], -0.15 if metric == 'avg_epa' else -0.4, \n",
    "                           f'Saved: {saved:.2f} {unit}', \n",
    "                           ha='center', va='top', fontsize=9, fontweight='bold',\n",
    "                           color='#1a5276', \n",
    "                           bbox=dict(boxstyle='round,pad=0.2', facecolor='#d5f5e3', edgecolor='#1e8449', alpha=0.9))\n",
    "            \n",
    "            # Formatting\n",
    "            ax.set_xticks(x_positions)\n",
    "            ax.set_xticklabels(bands, fontsize=10)\n",
    "            ax.set_xlabel('Starting Distance Band', fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel(ylabel, fontsize=12, fontweight='bold')\n",
    "            ax.set_title(title, fontsize=14, fontweight='bold', pad=15)\n",
    "            ax.legend(loc='upper right', fontsize=9)\n",
    "            ax.axhline(0, color='black', linewidth=0.5)\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # Set y-limits with padding for labels\n",
    "            if metric == 'avg_epa':\n",
    "                ax.set_ylim(bottom=-0.6, top=max_height + 0.5)\n",
    "            else:\n",
    "                ax.set_ylim(bottom=-1.0, top=max_height + 0.8)\n",
    "        \n",
    "        # Overall title\n",
    "        fig.suptitle('The Payoff of Eraser: High-Effort Defenders Save Points & Yards',\n",
    "                     fontsize=16, fontweight='bold', y=0.98)\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0.05, 1, 0.95])  # Leave room for labels and title\n",
    "        output_path = os.path.join(self.output_dir, 'V4_Effort_Impact_Chart.png')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "class TableGenerator:\n",
    "    def __init__(self, suumary_df: str):\n",
    "        self.df = suumary_df\n",
    "        \n",
    "        # We exclude Pass Rushers who occasionally drop into coverage\n",
    "        self.df = self.df[self.df['player_role'].isin([\n",
    "            'Defensive Coverage', 'Cornerback', 'Safety', 'Linebacker'])]\n",
    "\n",
    "    def generate_quadrant_counts(self):\n",
    "        \"\"\"\n",
    "        TABLE 2: Quadrant Counts Table\n",
    "        Buckets plays into the 4 Matrix Outcomes.\n",
    "        \"\"\"\n",
    "        df = self.df.copy()\n",
    "\n",
    "        # Define Thresholds\n",
    "        OPEN_THRESH = 3.0\n",
    "        CLOSED_THRESH = 1.5\n",
    "\n",
    "        conditions = [\n",
    "            (df['dist_at_throw'] >= OPEN_THRESH) & (df['dist_at_arrival'] <= CLOSED_THRESH), # Open -> Closed\n",
    "            (df['dist_at_throw'] < OPEN_THRESH) & (df['dist_at_arrival'] <= CLOSED_THRESH),  # Tight -> Closed\n",
    "            (df['dist_at_throw'] < OPEN_THRESH) & (df['dist_at_arrival'] > CLOSED_THRESH),  # Tight -> Open\n",
    "            (df['dist_at_throw'] >= OPEN_THRESH) & (df['dist_at_arrival'] > CLOSED_THRESH)  # Open -> Open\n",
    "        ]\n",
    "        \n",
    "        choices = ['Eraser (The Cleanup)', 'Lockdown (The Blanket)', 'Lost Step (The Beat)', 'Liability (The Void)']\n",
    "        \n",
    "        df['quadrant'] = np.select(conditions, choices, default='Neutral/Zone Drift')\n",
    "        \n",
    "        summary = df.groupby('quadrant').agg(\n",
    "            play_count=('play_id', 'count'),\n",
    "            avg_vis=('vis_score', 'mean'),\n",
    "            avg_ceoe=('ceoe_score', 'mean')\n",
    "        ).reset_index()\n",
    "\n",
    "        summary['avg_vis'] = summary['avg_vis'].round(2)\n",
    "        summary['avg_ceoe'] = summary['avg_ceoe'].round(3)\n",
    "\n",
    "        return summary.sort_values('avg_vis', ascending=False)\n",
    "\n",
    "    def generate_shrunk_leaderboard(self, min_snaps=15, prior_m=20):\n",
    "        \"\"\"\n",
    "        Bayesian Shrinkage with Names.\n",
    "        \"\"\"\n",
    "        # Positional Priors\n",
    "        pos_stats = self.df.groupby('player_position')['ceoe_score'].mean().to_dict()\n",
    "\n",
    "        # Add player_name to grouping\n",
    "        group_cols = ['nfl_id', 'player_position', 'player_role']\n",
    "        if 'player_name' in self.df.columns:\n",
    "            group_cols.insert(1, 'player_name')\n",
    "\n",
    "        player_stats = self.df.groupby(group_cols).agg(\n",
    "            snaps=('play_id', 'count'),\n",
    "            raw_ceoe=('ceoe_score', 'mean'),\n",
    "            avg_vis=('vis_score', 'mean'),\n",
    "            avg_start=('p_dist_at_throw', 'mean')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Shrinkage\n",
    "        def apply_shrinkage(row):\n",
    "            prior_mu = pos_stats.get(row['player_position'], 0.0)\n",
    "            n = row['snaps']\n",
    "            m = prior_m\n",
    "            shrunk = ((n * row['raw_ceoe']) + (m * prior_mu)) / (n + m)\n",
    "            return shrunk\n",
    "\n",
    "        player_stats['shrunk_ceoe'] = player_stats.apply(apply_shrinkage, axis=1)\n",
    "\n",
    "        qualified = player_stats[player_stats['snaps'] >= min_snaps].copy()     \n",
    "        qualified['shrunk_ceoe'] = qualified['shrunk_ceoe'].round(3)\n",
    "        qualified['raw_ceoe'] = qualified['raw_ceoe'].round(3)\n",
    "        qualified['avg_vis'] = qualified['avg_vis'].round(2)\n",
    "        qualified['avg_start'] = qualified['avg_start'].round(1)\n",
    "\n",
    "        top_erasers = qualified.sort_values('shrunk_ceoe', ascending=False).head(10)\n",
    "        \n",
    "        return top_erasers\n",
    "\n",
    "    def generate_damage_control_validation(self):\n",
    "        \"\"\"\n",
    "        Damage Control Validation (YAC & EPA).\n",
    "\n",
    "        Hypothesis: On COMPLETED passes, higher VIS (better closing) \n",
    "        should correlate with LOWER YAC and LOWER EPA (better for defense).\n",
    "        \"\"\"\n",
    "        df = self.df.copy()\n",
    "        \n",
    "        # Filter for Completions Only (Where YAC exists)\n",
    "        completed = df[df['pass_result'] == 'C'].copy()\n",
    "\n",
    "        # Derive YAC\n",
    "        # YAC = Total Yards - Air Yards\n",
    "        completed['yac'] = completed['yards_gained'] - completed['pass_length']\n",
    "        \n",
    "        # Bin Start Distance (Context Control)\n",
    "        # We only care about Medium/High Voids where YAC is a threat.\n",
    "        bins = [3, 6, 10, 100]\n",
    "        labels = ['Medium (3-6)', 'High Void (6-10)', 'Deep (10+)']\n",
    "        completed['start_band'] = pd.cut(completed['p_dist_at_throw'], bins=bins, labels=labels)\n",
    "        \n",
    "        # Bin VIS Score (The Independent Variable)\n",
    "        # Low Effort vs. High Effort closing\n",
    "        vis_bins = [-np.inf, 0, 3, np.inf]\n",
    "        vis_labels = ['Negative (Lost Gap)', 'Moderate (0-3)', 'High Eraser (3+)']\n",
    "        completed['vis_bucket'] = pd.cut(completed['vis_score'], bins=vis_bins, labels=vis_labels)\n",
    "\n",
    "        damage_control = completed.groupby(['start_band', 'vis_bucket'], observed=False).agg(\n",
    "            count=('play_id', 'count'),\n",
    "            avg_yac=('yac', 'mean'),\n",
    "            avg_epa=('expected_points_added', 'mean') # Lower is better for defense\n",
    "        ).reset_index()\n",
    "\n",
    "        # Pivot for YAC (The Primary Proof)\n",
    "        yac_pivot = damage_control.pivot(index='start_band', columns='vis_bucket', values='avg_yac')\n",
    "        \n",
    "        # Calculate the \"Savings\" (Difference between Negative VIS and High Eraser)\n",
    "        yac_pivot['YAC_Savings'] = yac_pivot['Negative (Lost Gap)'] - yac_pivot['High Eraser (3+)']\n",
    "        \n",
    "        return yac_pivot.round(2)\n",
    "\n",
    "    def generate_epa_savings(self):\n",
    "        \"\"\"\n",
    "        EPA Savings Table (Quartile Approach).\n",
    "\n",
    "        Shows how much Expected Points high-effort defenders save vs low-effort defenders.\n",
    "        \n",
    "        Focuses on COMPLETED passes where EPA damage occurs.\n",
    "        \"\"\"\n",
    "        df = self.df.copy()\n",
    "        \n",
    "        completed = df[df['pass_result'] == 'C'].copy()\n",
    "        \n",
    "        # Create Start Distance bands\n",
    "        dist_bins = [0, 3, 6, 10, 100]\n",
    "        dist_labels = ['Tight (0-3)', 'Medium (3-6)', 'High Void (6-10)', 'Exempt (10+)']\n",
    "        completed['start_band'] = pd.cut(completed['p_dist_at_throw'], bins=dist_bins, labels=dist_labels)\n",
    "        \n",
    "        # Calculate VIS quartiles WITHIN each start band\n",
    "        # This avoids NaNs by making \"effort\" relative to what's possible from each start position\n",
    "        def get_quartile_label(group):\n",
    "            q25 = group['vis_score'].quantile(0.25)\n",
    "            q75 = group['vis_score'].quantile(0.75)\n",
    "            \n",
    "            conditions = [\n",
    "                group['vis_score'] <= q25,\n",
    "                group['vis_score'] >= q75\n",
    "            ]\n",
    "            choices = ['Low Effort (Q1)', 'High Effort (Q4)']\n",
    "            group['effort_bucket'] = np.select(conditions, choices, default='Middle (Q2-Q3)')\n",
    "            return group\n",
    "        \n",
    "        completed = completed.groupby('start_band', group_keys=False, observed=False).apply(get_quartile_label)\n",
    "        \n",
    "        # Filter to only Q1 and Q4 for clean comparison\n",
    "        extremes = completed[completed['effort_bucket'].isin(['Low Effort (Q1)', 'High Effort (Q4)'])]\n",
    "        \n",
    "        epa_table = extremes.groupby(['start_band', 'effort_bucket'], observed=False).agg(\n",
    "            play_count=('play_id', 'count'),\n",
    "            avg_epa=('expected_points_added', 'mean')\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Pivot for clear comparison\n",
    "        epa_pivot = epa_table.pivot(index='start_band', columns='effort_bucket', values='avg_epa')\n",
    "        \n",
    "        # Calculate EPA Saved (Low Effort EPA - High Effort EPA)\n",
    "        # Positive = High effort defenders saved points\n",
    "        if 'Low Effort (Q1)' in epa_pivot.columns and 'High Effort (Q4)' in epa_pivot.columns:\n",
    "            epa_pivot['EPA_Saved'] = epa_pivot['Low Effort (Q1)'] - epa_pivot['High Effort (Q4)']\n",
    "        \n",
    "        count_pivot = epa_table.pivot(index='start_band', columns='effort_bucket', values='play_count')\n",
    "        epa_pivot['Plays_Compared'] = count_pivot.sum(axis=1)\n",
    "        \n",
    "        col_order = ['Low Effort (Q1)', 'High Effort (Q4)', 'EPA_Saved', 'Plays_Compared']\n",
    "        epa_pivot = epa_pivot[[c for c in col_order if c in epa_pivot.columns]]\n",
    "        \n",
    "        return epa_pivot.round(3)\n",
    "\n",
    "    def generate_position_breakdown(self):\n",
    "        \"\"\"\n",
    "        Position Breakdown - \"Differe Players Roles\"\n",
    "\n",
    "        Shows which position groups are best suited for the Eraser role.\n",
    "        \"\"\"\n",
    "        df = self.df.copy()\n",
    "        \n",
    "        # Define Eraser criteria (derived from start/end distances, not void_type)\n",
    "        OPEN_THRESH = 6.0\n",
    "        CLOSED_THRESH = 2.0\n",
    "        df['is_eraser_play'] = (df['p_dist_at_throw'] >= OPEN_THRESH) & (df['dist_at_arrival'] <= CLOSED_THRESH)\n",
    "        \n",
    "        position_stats = df.groupby('player_position').agg(\n",
    "            play_count=('play_id', 'count'),\n",
    "            avg_start_dist=('p_dist_at_throw', 'mean'),\n",
    "            avg_end_dist=('dist_at_arrival', 'mean'),\n",
    "            avg_vis=('vis_score', 'mean'),\n",
    "            eraser_plays=('is_eraser_play', 'sum')\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Calculate Eraser Rate (% of plays where they achieved Eraser outcome)\n",
    "        position_stats['eraser_rate'] = (position_stats['eraser_plays'] / position_stats['play_count'] * 100).round(1)\n",
    "        \n",
    "        # Filter for positions with meaningful sample size\n",
    "        position_stats = position_stats[position_stats['play_count'] >= 50].copy()\n",
    "        \n",
    "        # Derive Eraser Archetype based on behavior patterns\n",
    "        def assign_archetype(row):\n",
    "            avg_start = row['avg_start_dist']\n",
    "            avg_vis = row['avg_vis']\n",
    "            eraser_rate = row['eraser_rate']\n",
    "            \n",
    "            # Primary Eraser: Deep starters who close aggressively\n",
    "            if avg_start >= 8 and avg_vis >= 1.5:\n",
    "                return \" Primary Eraser\"\n",
    "            # Secondary Eraser: Medium depth with good closing\n",
    "            elif avg_start >= 6 and avg_vis >= 1.0:\n",
    "                return \" Secondary Eraser\"\n",
    "            # Lockdown: Tight coverage specialists (low start = already close)\n",
    "            elif avg_start < 5 and avg_vis < 0.5:\n",
    "                return \" Lockdown Focus\"\n",
    "            # Situational: High eraser rate despite moderate metrics\n",
    "            elif eraser_rate >= 5:\n",
    "                return \" Situational Eraser\"\n",
    "            else:\n",
    "                return \" Zone Support\"\n",
    "        \n",
    "        position_stats['archetype'] = position_stats.apply(assign_archetype, axis=1)\n",
    "        \n",
    "        position_stats['avg_start_dist'] = position_stats['avg_start_dist'].round(1)\n",
    "        position_stats['avg_end_dist'] = position_stats['avg_end_dist'].round(1)\n",
    "        position_stats['avg_vis'] = position_stats['avg_vis'].round(2)\n",
    "        \n",
    "        # Select and order columns for output\n",
    "        # TODO: bring them from schema.\n",
    "        output_cols = ['player_position', 'play_count', 'avg_start_dist', 'avg_end_dist',\n",
    "                       'avg_vis', 'eraser_rate', 'archetype']\n",
    "        \n",
    "        return position_stats[output_cols].sort_values('avg_start_dist', ascending=False)\n",
    "\n",
    "    def generate_void_effect_size(self):\n",
    "        \"\"\"\n",
    "        Void Effect Size Analysis\n",
    "        Shows completion %, EPA, and YAC by S_throw band with effect size \n",
    "        ( from Tight baseline) to quantify the jump in difficulty.\n",
    "        \"\"\"\n",
    "        df = self.df.copy()\n",
    "        \n",
    "        # Define S_throw bands based on dist_at_throw (original separation)\n",
    "        dist_col = 'p_dist_at_throw' if 'p_dist_at_throw' in df.columns else 'dist_at_throw'\n",
    "        \n",
    "        bins = [0, 2, 6, 10, float('inf')]\n",
    "        labels = ['Tight (0-2 yds)', 'Medium (3-6 yds)', 'High Void (6-10 yds)', 'Deep (10+ yds)']\n",
    "        df['start_band'] = pd.cut(df[dist_col], bins=bins, labels=labels, include_lowest=True)\n",
    "        \n",
    "        # Derive YAC for completions (yards_gained - pass_length)\n",
    "        df['yac'] = df['yards_gained'] - df['pass_length']\n",
    "        \n",
    "        # Calculate metrics by band\n",
    "        band_stats = df.groupby('start_band', observed=False).agg(\n",
    "            play_count=('play_id', 'count'),\n",
    "            completions=('pass_result', lambda x: (x == 'C').sum()),\n",
    "            avg_epa=('expected_points_added', 'mean')\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Calculate YAC separately (only for completions)\n",
    "        completed = df[df['pass_result'] == 'C']\n",
    "        yac_by_band = completed.groupby('start_band', observed=False)['yac'].mean().reset_index()\n",
    "        yac_by_band.columns = ['start_band', 'avg_yac']\n",
    "\n",
    "        band_stats = band_stats.merge(yac_by_band, on='start_band', how='left')\n",
    "        \n",
    "        # Calculate completion percentage\n",
    "        band_stats['completion_pct'] = (band_stats['completions'] / band_stats['play_count'] * 100).round(1)\n",
    "        \n",
    "        # Calculate effect size ( from Tight baseline)\n",
    "        tight_completion = band_stats.loc[band_stats['start_band'] == 'Tight (0-2 yds)', 'completion_pct'].values\n",
    "        if len(tight_completion) > 0:\n",
    "            tight_baseline = tight_completion[0]\n",
    "            band_stats['delta_from_tight'] = band_stats['completion_pct'] - tight_baseline\n",
    "            band_stats['delta_from_tight'] = band_stats['delta_from_tight'].apply(\n",
    "                lambda x: f\"+{x:.1f}pp\" if x > 0 else (\"\" if x == 0 else f\"{x:.1f}pp\")\n",
    "            )\n",
    "        else:\n",
    "            band_stats['delta_from_tight'] = \"\"\n",
    "        \n",
    "        # Format other columns\n",
    "        band_stats['avg_epa'] = band_stats['avg_epa'].round(2)\n",
    "        band_stats['avg_yac'] = band_stats['avg_yac'].round(1)\n",
    "        band_stats['completion_pct'] = band_stats['completion_pct'].apply(lambda x: f\"{x}%\")\n",
    "        \n",
    "        output = band_stats[['start_band', 'play_count', 'completion_pct', 'delta_from_tight', 'avg_epa', 'avg_yac']]\n",
    "        output.columns = ['S_throw Band', 'Play Count', 'Completion %', ' from Tight', 'Avg EPA Allowed', 'Avg YAC']\n",
    "        \n",
    "        return output\n",
    "\n",
    "def run_visual_pipeline(SUMMARY_FILE=None, TRACKING_FILE=None, OUTPUT_DIR=None):\n",
    "\n",
    "    vis_cfg = VisPipelineConfig(\n",
    "        SUMMARY_FILE=SUMMARY_FILE or vis_config.SUMMARY_FILE,\n",
    "        TRACKING_FILE=TRACKING_FILE or vis_config.TRACKING_FILE,\n",
    "        OUTPUT_DIR=OUTPUT_DIR or vis_config.OUTPUT_DIR\n",
    "    )\n",
    "    \n",
    "    summary_path = vis_cfg.SUMMARY_FILE\n",
    "    tracking_path = vis_cfg.TRACKING_FILE\n",
    "    output_dir = vis_cfg.OUTPUT_DIR\n",
    "\n",
    "    loader = DataLoader(summary_path, tracking_path)\n",
    "    summary_df, frames_df = loader.load_data()\n",
    "    \n",
    "    # Story Engine (Logic & Stats)\n",
    "    story = StoryDataEngine(summary_df, frames_df)\n",
    "    cast_dict = story.cast_archetypes()\n",
    "    \n",
    "    # Get Comparisons\n",
    "    fs_contrast = story.get_position_contrast('FS')\n",
    "\n",
    "    # Visual Engine (Static Charts)\n",
    "    viz = StoryVisualEngine(summary_df, frames_df, output_dir)\n",
    "    viz.plot_eraser_landscape(cast_dict) \n",
    "    viz.plot_race_charts(cast_dict)\n",
    "    viz.plot_coverage_heatmap()\n",
    "    viz.plot_effort_impact_chart()\n",
    "\n",
    "    # Animation Engine (Video Rendering)\n",
    "    animator = AnimationEngine(summary_df, frames_df, output_dir)\n",
    "\n",
    "    # Render Top FS Eraser\n",
    "    if fs_contrast['top']:\n",
    "        animator.generate_video(\n",
    "            game_id=fs_contrast['top']['game_id'], \n",
    "            play_id=fs_contrast['top']['play_id'], \n",
    "            eraser_id=fs_contrast['top']['nfl_id'], \n",
    "            filename=\"Figure_Top_FS_Eraser.gif\" \n",
    "        )\n",
    "\n",
    "    # Render Bottom FS Eraser\n",
    "    if fs_contrast['bottom']:\n",
    "        animator.generate_video(\n",
    "            game_id=fs_contrast['bottom']['game_id'], \n",
    "            play_id=fs_contrast['bottom']['play_id'], \n",
    "            eraser_id=fs_contrast['bottom']['nfl_id'], \n",
    "            filename=\"Figure_Bottom_FS_Eraser.gif\" \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09e055ec-72fd-448f-af75-31bc4d5d6f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def print_header(title, subtitle=\"\"):\n",
    "    html = f\"\"\"\n",
    "    <div style=\"background-color:#1a1a2e; padding:15px; border-radius:5px; margin-top:30px; margin-bottom:10px; border-left: 5px solid #00ff88;\">\n",
    "        <h3 style=\"color:white; margin:0; font-family:sans-serif;\">{title}</h3>\n",
    "        <p style=\"color:#a5b1c2; margin:0; font-size:12px;\">{subtitle}</p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html))\n",
    "\n",
    "gen = TableGenerator(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab63286-413f-46a9-a5e5-489ccf4669fe",
   "metadata": {},
   "source": [
    "### Generate Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4bf545a-495b-4103-a158-05d3488be8b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"background-color:#1a1a2e; padding:15px; border-radius:5px; margin-top:30px; margin-bottom:10px; border-left: 5px solid #00ff88;\">\n",
       "        <h3 style=\"color:white; margin:0; font-family:sans-serif;\">1. The Eraser Leaderboard</h3>\n",
       "        <p style=\"color:#a5b1c2; margin:0; font-size:12px;\">Bayesian Shrinkage (m=20) | Top 10 Defenders</p>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7fd32 th {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_7fd32_row0_col0, #T_7fd32_row0_col1, #T_7fd32_row0_col2, #T_7fd32_row0_col3, #T_7fd32_row0_col4, #T_7fd32_row0_col5, #T_7fd32_row0_col6, #T_7fd32_row0_col7, #T_7fd32_row0_col8, #T_7fd32_row1_col0, #T_7fd32_row1_col1, #T_7fd32_row1_col2, #T_7fd32_row1_col3, #T_7fd32_row1_col4, #T_7fd32_row1_col5, #T_7fd32_row1_col6, #T_7fd32_row1_col7, #T_7fd32_row1_col8, #T_7fd32_row2_col0, #T_7fd32_row2_col1, #T_7fd32_row2_col2, #T_7fd32_row2_col3, #T_7fd32_row2_col4, #T_7fd32_row2_col5, #T_7fd32_row2_col6, #T_7fd32_row2_col7, #T_7fd32_row2_col8, #T_7fd32_row3_col0, #T_7fd32_row3_col1, #T_7fd32_row3_col2, #T_7fd32_row3_col3, #T_7fd32_row3_col4, #T_7fd32_row3_col5, #T_7fd32_row3_col6, #T_7fd32_row3_col7, #T_7fd32_row3_col8, #T_7fd32_row4_col0, #T_7fd32_row4_col1, #T_7fd32_row4_col2, #T_7fd32_row4_col3, #T_7fd32_row4_col4, #T_7fd32_row4_col5, #T_7fd32_row4_col6, #T_7fd32_row4_col7, #T_7fd32_row4_col8, #T_7fd32_row5_col0, #T_7fd32_row5_col1, #T_7fd32_row5_col2, #T_7fd32_row5_col3, #T_7fd32_row5_col4, #T_7fd32_row5_col5, #T_7fd32_row5_col6, #T_7fd32_row5_col7, #T_7fd32_row5_col8, #T_7fd32_row6_col0, #T_7fd32_row6_col1, #T_7fd32_row6_col2, #T_7fd32_row6_col3, #T_7fd32_row6_col4, #T_7fd32_row6_col5, #T_7fd32_row6_col6, #T_7fd32_row6_col7, #T_7fd32_row6_col8, #T_7fd32_row7_col0, #T_7fd32_row7_col1, #T_7fd32_row7_col2, #T_7fd32_row7_col3, #T_7fd32_row7_col4, #T_7fd32_row7_col5, #T_7fd32_row7_col6, #T_7fd32_row7_col7, #T_7fd32_row7_col8, #T_7fd32_row8_col0, #T_7fd32_row8_col1, #T_7fd32_row8_col2, #T_7fd32_row8_col3, #T_7fd32_row8_col4, #T_7fd32_row8_col5, #T_7fd32_row8_col6, #T_7fd32_row8_col7, #T_7fd32_row8_col8, #T_7fd32_row9_col0, #T_7fd32_row9_col1, #T_7fd32_row9_col2, #T_7fd32_row9_col3, #T_7fd32_row9_col4, #T_7fd32_row9_col5, #T_7fd32_row9_col6, #T_7fd32_row9_col7, #T_7fd32_row9_col8 {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7fd32\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7fd32_level0_col0\" class=\"col_heading level0 col0\" >nfl_id</th>\n",
       "      <th id=\"T_7fd32_level0_col1\" class=\"col_heading level0 col1\" >player_name</th>\n",
       "      <th id=\"T_7fd32_level0_col2\" class=\"col_heading level0 col2\" >player_position</th>\n",
       "      <th id=\"T_7fd32_level0_col3\" class=\"col_heading level0 col3\" >player_role</th>\n",
       "      <th id=\"T_7fd32_level0_col4\" class=\"col_heading level0 col4\" >snaps</th>\n",
       "      <th id=\"T_7fd32_level0_col5\" class=\"col_heading level0 col5\" >raw_ceoe</th>\n",
       "      <th id=\"T_7fd32_level0_col6\" class=\"col_heading level0 col6\" >avg_vis</th>\n",
       "      <th id=\"T_7fd32_level0_col7\" class=\"col_heading level0 col7\" >avg_start</th>\n",
       "      <th id=\"T_7fd32_level0_col8\" class=\"col_heading level0 col8\" >shrunk_ceoe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7fd32_level0_row0\" class=\"row_heading level0 row0\" >337</th>\n",
       "      <td id=\"T_7fd32_row0_col0\" class=\"data row0 col0\" >53494.000000</td>\n",
       "      <td id=\"T_7fd32_row0_col1\" class=\"data row0 col1\" >Andre Cisco</td>\n",
       "      <td id=\"T_7fd32_row0_col2\" class=\"data row0 col2\" >FS</td>\n",
       "      <td id=\"T_7fd32_row0_col3\" class=\"data row0 col3\" >Defensive Coverage</td>\n",
       "      <td id=\"T_7fd32_row0_col4\" class=\"data row0 col4\" >21</td>\n",
       "      <td id=\"T_7fd32_row0_col5\" class=\"data row0 col5\" >+2.750</td>\n",
       "      <td id=\"T_7fd32_row0_col6\" class=\"data row0 col6\" >+4.95</td>\n",
       "      <td id=\"T_7fd32_row0_col7\" class=\"data row0 col7\" >11.5</td>\n",
       "      <td id=\"T_7fd32_row0_col8\" class=\"data row0 col8\" >+1.408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7fd32_level0_row1\" class=\"row_heading level0 row1\" >387</th>\n",
       "      <td id=\"T_7fd32_row1_col0\" class=\"data row1 col0\" >54496.000000</td>\n",
       "      <td id=\"T_7fd32_row1_col1\" class=\"data row1 col1\" >Daxton Hill</td>\n",
       "      <td id=\"T_7fd32_row1_col2\" class=\"data row1 col2\" >CB</td>\n",
       "      <td id=\"T_7fd32_row1_col3\" class=\"data row1 col3\" >Defensive Coverage</td>\n",
       "      <td id=\"T_7fd32_row1_col4\" class=\"data row1 col4\" >35</td>\n",
       "      <td id=\"T_7fd32_row1_col5\" class=\"data row1 col5\" >+1.972</td>\n",
       "      <td id=\"T_7fd32_row1_col6\" class=\"data row1 col6\" >+2.45</td>\n",
       "      <td id=\"T_7fd32_row1_col7\" class=\"data row1 col7\" >12.1</td>\n",
       "      <td id=\"T_7fd32_row1_col8\" class=\"data row1 col8\" >+1.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7fd32_level0_row2\" class=\"row_heading level0 row2\" >354</th>\n",
       "      <td id=\"T_7fd32_row2_col0\" class=\"data row2 col0\" >53554.000000</td>\n",
       "      <td id=\"T_7fd32_row2_col1\" class=\"data row2 col1\" >Camryn Bynum</td>\n",
       "      <td id=\"T_7fd32_row2_col2\" class=\"data row2 col2\" >FS</td>\n",
       "      <td id=\"T_7fd32_row2_col3\" class=\"data row2 col3\" >Defensive Coverage</td>\n",
       "      <td id=\"T_7fd32_row2_col4\" class=\"data row2 col4\" >31</td>\n",
       "      <td id=\"T_7fd32_row2_col5\" class=\"data row2 col5\" >+1.730</td>\n",
       "      <td id=\"T_7fd32_row2_col6\" class=\"data row2 col6\" >+4.80</td>\n",
       "      <td id=\"T_7fd32_row2_col7\" class=\"data row2 col7\" >11.9</td>\n",
       "      <td id=\"T_7fd32_row2_col8\" class=\"data row2 col8\" >+1.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7fd32_level0_row3\" class=\"row_heading level0 row3\" >311</th>\n",
       "      <td id=\"T_7fd32_row3_col0\" class=\"data row3 col0\" >52844.000000</td>\n",
       "      <td id=\"T_7fd32_row3_col1\" class=\"data row3 col1\" >Christian Rozeboom</td>\n",
       "      <td id=\"T_7fd32_row3_col2\" class=\"data row3 col2\" >ILB</td>\n",
       "      <td id=\"T_7fd32_row3_col3\" class=\"data row3 col3\" >Defensive Coverage</td>\n",
       "      <td id=\"T_7fd32_row3_col4\" class=\"data row3 col4\" >16</td>\n",
       "      <td id=\"T_7fd32_row3_col5\" class=\"data row3 col5\" >+2.330</td>\n",
       "      <td id=\"T_7fd32_row3_col6\" class=\"data row3 col6\" >+2.86</td>\n",
       "      <td id=\"T_7fd32_row3_col7\" class=\"data row3 col7\" >8.5</td>\n",
       "      <td id=\"T_7fd32_row3_col8\" class=\"data row3 col8\" >+1.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7fd32_level0_row4\" class=\"row_heading level0 row4\" >60</th>\n",
       "      <td id=\"T_7fd32_row4_col0\" class=\"data row4 col0\" >43351.000000</td>\n",
       "      <td id=\"T_7fd32_row4_col1\" class=\"data row4 col1\" >James Bradberry</td>\n",
       "      <td id=\"T_7fd32_row4_col2\" class=\"data row4 col2\" >CB</td>\n",
       "      <td id=\"T_7fd32_row4_col3\" class=\"data row4 col3\" >Defensive Coverage</td>\n",
       "      <td id=\"T_7fd32_row4_col4\" class=\"data row4 col4\" >21</td>\n",
       "      <td id=\"T_7fd32_row4_col5\" class=\"data row4 col5\" >+1.845</td>\n",
       "      <td id=\"T_7fd32_row4_col6\" class=\"data row4 col6\" >+2.47</td>\n",
       "      <td id=\"T_7fd32_row4_col7\" class=\"data row4 col7\" >6.8</td>\n",
       "      <td id=\"T_7fd32_row4_col8\" class=\"data row4 col8\" >+0.945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7fd32_level0_row5\" class=\"row_heading level0 row5\" >4</th>\n",
       "      <td id=\"T_7fd32_row5_col0\" class=\"data row5 col0\" >37078.000000</td>\n",
       "      <td id=\"T_7fd32_row5_col1\" class=\"data row5 col1\" >Patrick Peterson</td>\n",
       "      <td id=\"T_7fd32_row5_col2\" class=\"data row5 col2\" >CB</td>\n",
       "      <td id=\"T_7fd32_row5_col3\" class=\"data row5 col3\" >Defensive Coverage</td>\n",
       "      <td id=\"T_7fd32_row5_col4\" class=\"data row5 col4\" >23</td>\n",
       "      <td id=\"T_7fd32_row5_col5\" class=\"data row5 col5\" >+1.762</td>\n",
       "      <td id=\"T_7fd32_row5_col6\" class=\"data row5 col6\" >+2.12</td>\n",
       "      <td id=\"T_7fd32_row5_col7\" class=\"data row5 col7\" >9.3</td>\n",
       "      <td id=\"T_7fd32_row5_col8\" class=\"data row5 col8\" >+0.942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7fd32_level0_row6\" class=\"row_heading level0 row6\" >19</th>\n",
       "      <td id=\"T_7fd32_row6_col0\" class=\"data row6 col0\" >40166.000000</td>\n",
       "      <td id=\"T_7fd32_row6_col1\" class=\"data row6 col1\" >Jordan Poyer</td>\n",
       "      <td id=\"T_7fd32_row6_col2\" class=\"data row6 col2\" >SS</td>\n",
       "      <td id=\"T_7fd32_row6_col3\" class=\"data row6 col3\" >Defensive Coverage</td>\n",
       "      <td id=\"T_7fd32_row6_col4\" class=\"data row6 col4\" >16</td>\n",
       "      <td id=\"T_7fd32_row6_col5\" class=\"data row6 col5\" >+2.035</td>\n",
       "      <td id=\"T_7fd32_row6_col6\" class=\"data row6 col6\" >+2.97</td>\n",
       "      <td id=\"T_7fd32_row6_col7\" class=\"data row6 col7\" >9.8</td>\n",
       "      <td id=\"T_7fd32_row6_col8\" class=\"data row6 col8\" >+0.904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7fd32_level0_row7\" class=\"row_heading level0 row7\" >176</th>\n",
       "      <td id=\"T_7fd32_row7_col0\" class=\"data row7 col0\" >46527.000000</td>\n",
       "      <td id=\"T_7fd32_row7_col1\" class=\"data row7 col1\" >Frankie Luvu</td>\n",
       "      <td id=\"T_7fd32_row7_col2\" class=\"data row7 col2\" >OLB</td>\n",
       "      <td id=\"T_7fd32_row7_col3\" class=\"data row7 col3\" >Defensive Coverage</td>\n",
       "      <td id=\"T_7fd32_row7_col4\" class=\"data row7 col4\" >20</td>\n",
       "      <td id=\"T_7fd32_row7_col5\" class=\"data row7 col5\" >+1.520</td>\n",
       "      <td id=\"T_7fd32_row7_col6\" class=\"data row7 col6\" >+1.72</td>\n",
       "      <td id=\"T_7fd32_row7_col7\" class=\"data row7 col7\" >7.5</td>\n",
       "      <td id=\"T_7fd32_row7_col8\" class=\"data row7 col8\" >+0.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7fd32_level0_row8\" class=\"row_heading level0 row8\" >10</th>\n",
       "      <td id=\"T_7fd32_row8_col0\" class=\"data row8 col0\" >38577.000000</td>\n",
       "      <td id=\"T_7fd32_row8_col1\" class=\"data row8 col1\" >Bobby Wagner</td>\n",
       "      <td id=\"T_7fd32_row8_col2\" class=\"data row8 col2\" >MLB</td>\n",
       "      <td id=\"T_7fd32_row8_col3\" class=\"data row8 col3\" >Defensive Coverage</td>\n",
       "      <td id=\"T_7fd32_row8_col4\" class=\"data row8 col4\" >26</td>\n",
       "      <td id=\"T_7fd32_row8_col5\" class=\"data row8 col5\" >+1.319</td>\n",
       "      <td id=\"T_7fd32_row8_col6\" class=\"data row8 col6\" >+1.85</td>\n",
       "      <td id=\"T_7fd32_row8_col7\" class=\"data row8 col7\" >8.1</td>\n",
       "      <td id=\"T_7fd32_row8_col8\" class=\"data row8 col8\" >+0.746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7fd32_level0_row9\" class=\"row_heading level0 row9\" >265</th>\n",
       "      <td id=\"T_7fd32_row9_col0\" class=\"data row9 col0\" >52445.000000</td>\n",
       "      <td id=\"T_7fd32_row9_col1\" class=\"data row9 col1\" >Kyle Dugger</td>\n",
       "      <td id=\"T_7fd32_row9_col2\" class=\"data row9 col2\" >SS</td>\n",
       "      <td id=\"T_7fd32_row9_col3\" class=\"data row9 col3\" >Defensive Coverage</td>\n",
       "      <td id=\"T_7fd32_row9_col4\" class=\"data row9 col4\" >19</td>\n",
       "      <td id=\"T_7fd32_row9_col5\" class=\"data row9 col5\" >+1.521</td>\n",
       "      <td id=\"T_7fd32_row9_col6\" class=\"data row9 col6\" >+2.70</td>\n",
       "      <td id=\"T_7fd32_row9_col7\" class=\"data row9 col7\" >9.7</td>\n",
       "      <td id=\"T_7fd32_row9_col8\" class=\"data row9 col8\" >+0.741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7a2dbeced100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- TABLE 1: SHRUNK LEADERBOARD ---\n",
    "df_leaderboard = gen.generate_shrunk_leaderboard()\n",
    "print_header(\"1. The Eraser Leaderboard\", \"Bayesian Shrinkage (m=20) | Top 10 Defenders\")\n",
    "\n",
    "st_leader = (df_leaderboard.style\n",
    "             .format({'shrunk_ceoe': '{:+.3f}', 'raw_ceoe': '{:+.3f}', 'avg_vis': '{:+.2f}', 'avg_start': '{:.1f}'})\n",
    "             .set_properties(**{'text-align': 'center'})\n",
    "             .set_table_styles([{'selector': 'th', 'props': [('text-align', 'center')]}])\n",
    ")\n",
    "display(st_leader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92424285-b0f9-4929-a98f-ac00da988e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"background-color:#1a1a2e; padding:15px; border-radius:5px; margin-top:30px; margin-bottom:10px; border-left: 5px solid #00ff88;\">\n",
       "        <h3 style=\"color:white; margin:0; font-family:sans-serif;\">2. Quadrant Matrix Summary</h3>\n",
       "        <p style=\"color:#a5b1c2; margin:0; font-size:12px;\">Distribution of Plays across the 4 Eraser Archetypes</p>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_aeb2c th {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_aeb2c_row0_col0, #T_aeb2c_row0_col1, #T_aeb2c_row0_col2, #T_aeb2c_row0_col3, #T_aeb2c_row1_col0, #T_aeb2c_row1_col1, #T_aeb2c_row1_col2, #T_aeb2c_row1_col3, #T_aeb2c_row2_col0, #T_aeb2c_row2_col1, #T_aeb2c_row2_col2, #T_aeb2c_row2_col3, #T_aeb2c_row3_col0, #T_aeb2c_row3_col1, #T_aeb2c_row3_col2, #T_aeb2c_row3_col3 {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_aeb2c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_aeb2c_level0_col0\" class=\"col_heading level0 col0\" >quadrant</th>\n",
       "      <th id=\"T_aeb2c_level0_col1\" class=\"col_heading level0 col1\" >play_count</th>\n",
       "      <th id=\"T_aeb2c_level0_col2\" class=\"col_heading level0 col2\" >avg_vis</th>\n",
       "      <th id=\"T_aeb2c_level0_col3\" class=\"col_heading level0 col3\" >avg_ceoe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_aeb2c_row0_col0\" class=\"data row0 col0\" >Eraser (The Cleanup)</td>\n",
       "      <td id=\"T_aeb2c_row0_col1\" class=\"data row0 col1\" >173</td>\n",
       "      <td id=\"T_aeb2c_row0_col2\" class=\"data row0 col2\" >+3.39</td>\n",
       "      <td id=\"T_aeb2c_row0_col3\" class=\"data row0 col3\" >+2.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_aeb2c_row1_col0\" class=\"data row1 col0\" >Lockdown (The Blanket)</td>\n",
       "      <td id=\"T_aeb2c_row1_col1\" class=\"data row1 col1\" >366</td>\n",
       "      <td id=\"T_aeb2c_row1_col2\" class=\"data row1 col2\" >+1.32</td>\n",
       "      <td id=\"T_aeb2c_row1_col3\" class=\"data row1 col3\" >+0.857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_aeb2c_row2_col0\" class=\"data row2 col0\" >Liability (The Void)</td>\n",
       "      <td id=\"T_aeb2c_row2_col1\" class=\"data row2 col1\" >2889</td>\n",
       "      <td id=\"T_aeb2c_row2_col2\" class=\"data row2 col2\" >+0.80</td>\n",
       "      <td id=\"T_aeb2c_row2_col3\" class=\"data row2 col3\" >-0.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_aeb2c_row3_col0\" class=\"data row3 col0\" >Lost Step (The Beat)</td>\n",
       "      <td id=\"T_aeb2c_row3_col1\" class=\"data row3 col1\" >1763</td>\n",
       "      <td id=\"T_aeb2c_row3_col2\" class=\"data row3 col2\" >+0.15</td>\n",
       "      <td id=\"T_aeb2c_row3_col3\" class=\"data row3 col3\" >-0.303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7a2dbe2d2450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- TABLE 2: QUADRANT SUMMARY ---\n",
    "df_quad = gen.generate_quadrant_counts()\n",
    "print_header(\"2. Quadrant Matrix Summary\", \"Distribution of Plays across the 4 Eraser Archetypes\")\n",
    "\n",
    "st_quad = (df_quad.style\n",
    "           .format({'avg_vis': '{:+.2f}', 'avg_ceoe': '{:+.3f}'})\n",
    "           .set_properties(**{'text-align': 'center'})\n",
    "           .set_table_styles([{'selector': 'th', 'props': [('text-align', 'center')]}])\n",
    "           .hide(axis='index')\n",
    ")\n",
    "display(st_quad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6663f5cd-a7c4-499a-9ae7-751d0ea9bd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"background-color:#1a1a2e; padding:15px; border-radius:5px; margin-top:30px; margin-bottom:10px; border-left: 5px solid #00ff88;\">\n",
       "        <h3 style=\"color:white; margin:0; font-family:sans-serif;\">3. Damage Control: YAC Analysis</h3>\n",
       "        <p style=\"color:#a5b1c2; margin:0; font-size:12px;\">Does High VIS reduce Yards After Catch?</p>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_dbe59 th {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_dbe59_row0_col0, #T_dbe59_row0_col1, #T_dbe59_row0_col2, #T_dbe59_row0_col3, #T_dbe59_row1_col0, #T_dbe59_row1_col1, #T_dbe59_row1_col2, #T_dbe59_row1_col3, #T_dbe59_row2_col0, #T_dbe59_row2_col1, #T_dbe59_row2_col2, #T_dbe59_row2_col3 {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_dbe59\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >vis_bucket</th>\n",
       "      <th id=\"T_dbe59_level0_col0\" class=\"col_heading level0 col0\" >Negative (Lost Gap)</th>\n",
       "      <th id=\"T_dbe59_level0_col1\" class=\"col_heading level0 col1\" >Moderate (0-3)</th>\n",
       "      <th id=\"T_dbe59_level0_col2\" class=\"col_heading level0 col2\" >High Eraser (3+)</th>\n",
       "      <th id=\"T_dbe59_level0_col3\" class=\"col_heading level0 col3\" >YAC_Savings</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >start_band</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_dbe59_level0_row0\" class=\"row_heading level0 row0\" >Medium (3-6)</th>\n",
       "      <td id=\"T_dbe59_row0_col0\" class=\"data row0 col0\" >4.77</td>\n",
       "      <td id=\"T_dbe59_row0_col1\" class=\"data row0 col1\" >2.64</td>\n",
       "      <td id=\"T_dbe59_row0_col2\" class=\"data row0 col2\" >1.43</td>\n",
       "      <td id=\"T_dbe59_row0_col3\" class=\"data row0 col3\" >+3.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dbe59_level0_row1\" class=\"row_heading level0 row1\" >High Void (6-10)</th>\n",
       "      <td id=\"T_dbe59_row1_col0\" class=\"data row1 col0\" >4.65</td>\n",
       "      <td id=\"T_dbe59_row1_col1\" class=\"data row1 col1\" >3.91</td>\n",
       "      <td id=\"T_dbe59_row1_col2\" class=\"data row1 col2\" >2.50</td>\n",
       "      <td id=\"T_dbe59_row1_col3\" class=\"data row1 col3\" >+2.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dbe59_level0_row2\" class=\"row_heading level0 row2\" >Deep (10+)</th>\n",
       "      <td id=\"T_dbe59_row2_col0\" class=\"data row2 col0\" >5.34</td>\n",
       "      <td id=\"T_dbe59_row2_col1\" class=\"data row2 col1\" >4.98</td>\n",
       "      <td id=\"T_dbe59_row2_col2\" class=\"data row2 col2\" >4.29</td>\n",
       "      <td id=\"T_dbe59_row2_col3\" class=\"data row2 col3\" >+1.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7a2dbde55d60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_yac = gen.generate_damage_control_validation()\n",
    "print_header(\"3. Damage Control: YAC Analysis\", \"Does High VIS reduce Yards After Catch?\")\n",
    "\n",
    "savings_col = 'YAC_Savings' if 'YAC_Savings' in df_yac.columns else None\n",
    "\n",
    "format_dict = {col: '{:.2f}' for col in df_yac.select_dtypes(include=['float', 'int']).columns}\n",
    "if savings_col:\n",
    "    format_dict[savings_col] = '{:+.2f}'\n",
    "\n",
    "st_yac = (df_yac.style\n",
    "          .format(format_dict)\n",
    "          .set_properties(**{'text-align': 'center'})\n",
    "          .set_table_styles([{'selector': 'th', 'props': [('text-align', 'center')]}])\n",
    ")\n",
    "display(st_yac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "feb80dd5-4737-46b6-90c0-1f7f290de43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23908/2635346856.py:1294: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  completed = completed.groupby('start_band', group_keys=False, observed=False).apply(get_quartile_label)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"background-color:#1a1a2e; padding:15px; border-radius:5px; margin-top:30px; margin-bottom:10px; border-left: 5px solid #00ff88;\">\n",
       "        <h3 style=\"color:white; margin:0; font-family:sans-serif;\">4. EPA Savings</h3>\n",
       "        <p style=\"color:#a5b1c2; margin:0; font-size:12px;\">Expected Points Saved by High-Effort (Q4) vs Low-Effort (Q1) Defenders</p>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_4bd74 th {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_4bd74_row0_col0, #T_4bd74_row0_col1, #T_4bd74_row0_col2, #T_4bd74_row0_col3, #T_4bd74_row1_col0, #T_4bd74_row1_col1, #T_4bd74_row1_col2, #T_4bd74_row1_col3, #T_4bd74_row2_col0, #T_4bd74_row2_col1, #T_4bd74_row2_col2, #T_4bd74_row2_col3, #T_4bd74_row3_col0, #T_4bd74_row3_col1, #T_4bd74_row3_col2, #T_4bd74_row3_col3 {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_4bd74\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >effort_bucket</th>\n",
       "      <th id=\"T_4bd74_level0_col0\" class=\"col_heading level0 col0\" >Low Effort (Q1)</th>\n",
       "      <th id=\"T_4bd74_level0_col1\" class=\"col_heading level0 col1\" >High Effort (Q4)</th>\n",
       "      <th id=\"T_4bd74_level0_col2\" class=\"col_heading level0 col2\" >EPA_Saved</th>\n",
       "      <th id=\"T_4bd74_level0_col3\" class=\"col_heading level0 col3\" >Plays_Compared</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >start_band</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4bd74_level0_row0\" class=\"row_heading level0 row0\" >Tight (0-3)</th>\n",
       "      <td id=\"T_4bd74_row0_col0\" class=\"data row0 col0\" >1.260</td>\n",
       "      <td id=\"T_4bd74_row0_col1\" class=\"data row0 col1\" >0.879</td>\n",
       "      <td id=\"T_4bd74_row0_col2\" class=\"data row0 col2\" >+0.381</td>\n",
       "      <td id=\"T_4bd74_row0_col3\" class=\"data row0 col3\" >228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4bd74_level0_row1\" class=\"row_heading level0 row1\" >Medium (3-6)</th>\n",
       "      <td id=\"T_4bd74_row1_col0\" class=\"data row1 col0\" >1.222</td>\n",
       "      <td id=\"T_4bd74_row1_col1\" class=\"data row1 col1\" >0.480</td>\n",
       "      <td id=\"T_4bd74_row1_col2\" class=\"data row1 col2\" >+0.742</td>\n",
       "      <td id=\"T_4bd74_row1_col3\" class=\"data row1 col3\" >504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4bd74_level0_row2\" class=\"row_heading level0 row2\" >High Void (6-10)</th>\n",
       "      <td id=\"T_4bd74_row2_col0\" class=\"data row2 col0\" >1.308</td>\n",
       "      <td id=\"T_4bd74_row2_col1\" class=\"data row2 col1\" >0.816</td>\n",
       "      <td id=\"T_4bd74_row2_col2\" class=\"data row2 col2\" >+0.492</td>\n",
       "      <td id=\"T_4bd74_row2_col3\" class=\"data row2 col3\" >600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4bd74_level0_row3\" class=\"row_heading level0 row3\" >Exempt (10+)</th>\n",
       "      <td id=\"T_4bd74_row3_col0\" class=\"data row3 col0\" >1.522</td>\n",
       "      <td id=\"T_4bd74_row3_col1\" class=\"data row3 col1\" >1.528</td>\n",
       "      <td id=\"T_4bd74_row3_col2\" class=\"data row3 col2\" >-0.005</td>\n",
       "      <td id=\"T_4bd74_row3_col3\" class=\"data row3 col3\" >466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7a2dbde55b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- TABLE 4: EPA SAVINGS ---\n",
    "df_epa = gen.generate_epa_savings()\n",
    "print_header(\"4. EPA Savings\", \"Expected Points Saved by High-Effort (Q4) vs Low-Effort (Q1) Defenders\")\n",
    "\n",
    "st_epa = (df_epa.style\n",
    "          .format({'EPA_Saved': '{:+.3f}', 'Low Effort (Q1)': '{:.3f}', 'High Effort (Q4)': '{:.3f}'})\n",
    "          .set_properties(**{'text-align': 'center'})\n",
    "          .set_table_styles([{'selector': 'th', 'props': [('text-align', 'center')]}])\n",
    ")\n",
    "display(st_epa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c923741-ff1a-4a30-b970-b3508687e273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"background-color:#1a1a2e; padding:15px; border-radius:5px; margin-top:30px; margin-bottom:10px; border-left: 5px solid #00ff88;\">\n",
       "        <h3 style=\"color:white; margin:0; font-family:sans-serif;\">5. Position Breakdown</h3>\n",
       "        <p style=\"color:#a5b1c2; margin:0; font-size:12px;\">Which positions generate the most Eraser events?</p>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_103fd th {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_103fd_row0_col0, #T_103fd_row0_col1, #T_103fd_row0_col2, #T_103fd_row0_col3, #T_103fd_row0_col4, #T_103fd_row0_col5, #T_103fd_row0_col6, #T_103fd_row1_col0, #T_103fd_row1_col1, #T_103fd_row1_col2, #T_103fd_row1_col3, #T_103fd_row1_col4, #T_103fd_row1_col5, #T_103fd_row1_col6, #T_103fd_row2_col0, #T_103fd_row2_col1, #T_103fd_row2_col2, #T_103fd_row2_col3, #T_103fd_row2_col4, #T_103fd_row2_col5, #T_103fd_row2_col6, #T_103fd_row3_col0, #T_103fd_row3_col1, #T_103fd_row3_col2, #T_103fd_row3_col3, #T_103fd_row3_col4, #T_103fd_row3_col5, #T_103fd_row3_col6, #T_103fd_row4_col0, #T_103fd_row4_col1, #T_103fd_row4_col2, #T_103fd_row4_col3, #T_103fd_row4_col4, #T_103fd_row4_col5, #T_103fd_row4_col6, #T_103fd_row5_col0, #T_103fd_row5_col1, #T_103fd_row5_col2, #T_103fd_row5_col3, #T_103fd_row5_col4, #T_103fd_row5_col5, #T_103fd_row5_col6, #T_103fd_row6_col0, #T_103fd_row6_col1, #T_103fd_row6_col2, #T_103fd_row6_col3, #T_103fd_row6_col4, #T_103fd_row6_col5, #T_103fd_row6_col6 {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_103fd\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_103fd_level0_col0\" class=\"col_heading level0 col0\" >player_position</th>\n",
       "      <th id=\"T_103fd_level0_col1\" class=\"col_heading level0 col1\" >play_count</th>\n",
       "      <th id=\"T_103fd_level0_col2\" class=\"col_heading level0 col2\" >avg_start_dist</th>\n",
       "      <th id=\"T_103fd_level0_col3\" class=\"col_heading level0 col3\" >avg_end_dist</th>\n",
       "      <th id=\"T_103fd_level0_col4\" class=\"col_heading level0 col4\" >avg_vis</th>\n",
       "      <th id=\"T_103fd_level0_col5\" class=\"col_heading level0 col5\" >eraser_rate</th>\n",
       "      <th id=\"T_103fd_level0_col6\" class=\"col_heading level0 col6\" >archetype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_103fd_row0_col0\" class=\"data row0 col0\" >FS</td>\n",
       "      <td id=\"T_103fd_row0_col1\" class=\"data row0 col1\" >820</td>\n",
       "      <td id=\"T_103fd_row0_col2\" class=\"data row0 col2\" >9.8</td>\n",
       "      <td id=\"T_103fd_row0_col3\" class=\"data row0 col3\" >7.9</td>\n",
       "      <td id=\"T_103fd_row0_col4\" class=\"data row0 col4\" >+1.89</td>\n",
       "      <td id=\"T_103fd_row0_col5\" class=\"data row0 col5\" >2.2%</td>\n",
       "      <td id=\"T_103fd_row0_col6\" class=\"data row0 col6\" > Primary Eraser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_103fd_row1_col0\" class=\"data row1 col0\" >SS</td>\n",
       "      <td id=\"T_103fd_row1_col1\" class=\"data row1 col1\" >628</td>\n",
       "      <td id=\"T_103fd_row1_col2\" class=\"data row1 col2\" >9.1</td>\n",
       "      <td id=\"T_103fd_row1_col3\" class=\"data row1 col3\" >7.9</td>\n",
       "      <td id=\"T_103fd_row1_col4\" class=\"data row1 col4\" >+1.22</td>\n",
       "      <td id=\"T_103fd_row1_col5\" class=\"data row1 col5\" >3.0%</td>\n",
       "      <td id=\"T_103fd_row1_col6\" class=\"data row1 col6\" > Secondary Eraser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_103fd_row2_col0\" class=\"data row2 col0\" >OLB</td>\n",
       "      <td id=\"T_103fd_row2_col1\" class=\"data row2 col1\" >494</td>\n",
       "      <td id=\"T_103fd_row2_col2\" class=\"data row2 col2\" >8.0</td>\n",
       "      <td id=\"T_103fd_row2_col3\" class=\"data row2 col3\" >7.5</td>\n",
       "      <td id=\"T_103fd_row2_col4\" class=\"data row2 col4\" >+0.44</td>\n",
       "      <td id=\"T_103fd_row2_col5\" class=\"data row2 col5\" >0.0%</td>\n",
       "      <td id=\"T_103fd_row2_col6\" class=\"data row2 col6\" > Zone Support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_103fd_row3_col0\" class=\"data row3 col0\" >MLB</td>\n",
       "      <td id=\"T_103fd_row3_col1\" class=\"data row3 col1\" >526</td>\n",
       "      <td id=\"T_103fd_row3_col2\" class=\"data row3 col2\" >7.9</td>\n",
       "      <td id=\"T_103fd_row3_col3\" class=\"data row3 col3\" >7.2</td>\n",
       "      <td id=\"T_103fd_row3_col4\" class=\"data row3 col4\" >+0.74</td>\n",
       "      <td id=\"T_103fd_row3_col5\" class=\"data row3 col5\" >1.0%</td>\n",
       "      <td id=\"T_103fd_row3_col6\" class=\"data row3 col6\" > Zone Support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_103fd_row4_col0\" class=\"data row4 col0\" >ILB</td>\n",
       "      <td id=\"T_103fd_row4_col1\" class=\"data row4 col1\" >687</td>\n",
       "      <td id=\"T_103fd_row4_col2\" class=\"data row4 col2\" >7.7</td>\n",
       "      <td id=\"T_103fd_row4_col3\" class=\"data row4 col3\" >7.2</td>\n",
       "      <td id=\"T_103fd_row4_col4\" class=\"data row4 col4\" >+0.45</td>\n",
       "      <td id=\"T_103fd_row4_col5\" class=\"data row4 col5\" >1.0%</td>\n",
       "      <td id=\"T_103fd_row4_col6\" class=\"data row4 col6\" > Zone Support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_103fd_row5_col0\" class=\"data row5 col0\" >DE</td>\n",
       "      <td id=\"T_103fd_row5_col1\" class=\"data row5 col1\" >66</td>\n",
       "      <td id=\"T_103fd_row5_col2\" class=\"data row5 col2\" >7.4</td>\n",
       "      <td id=\"T_103fd_row5_col3\" class=\"data row5 col3\" >7.3</td>\n",
       "      <td id=\"T_103fd_row5_col4\" class=\"data row5 col4\" >+0.06</td>\n",
       "      <td id=\"T_103fd_row5_col5\" class=\"data row5 col5\" >0.0%</td>\n",
       "      <td id=\"T_103fd_row5_col6\" class=\"data row5 col6\" > Zone Support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_103fd_row6_col0\" class=\"data row6 col0\" >CB</td>\n",
       "      <td id=\"T_103fd_row6_col1\" class=\"data row6 col1\" >1936</td>\n",
       "      <td id=\"T_103fd_row6_col2\" class=\"data row6 col2\" >6.5</td>\n",
       "      <td id=\"T_103fd_row6_col3\" class=\"data row6 col3\" >6.3</td>\n",
       "      <td id=\"T_103fd_row6_col4\" class=\"data row6 col4\" >+0.19</td>\n",
       "      <td id=\"T_103fd_row6_col5\" class=\"data row6 col5\" >1.4%</td>\n",
       "      <td id=\"T_103fd_row6_col6\" class=\"data row6 col6\" > Zone Support</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7a2dbde540b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_pos = gen.generate_position_breakdown()\n",
    "print_header(\"5. Position Breakdown\", \"Which positions generate the most Eraser events?\")\n",
    "\n",
    "st_pos = (df_pos.style\n",
    "          .format({'eraser_rate': '{:.1f}%', 'avg_start_dist': '{:.1f}', 'avg_end_dist': '{:.1f}', 'avg_vis': '{:+.2f}'})\n",
    "          .set_properties(**{'text-align': 'center'})\n",
    "          .set_table_styles([{'selector': 'th', 'props': [('text-align', 'center')]}])\n",
    "          .hide(axis='index')\n",
    ")\n",
    "display(st_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7befae61-fa4c-4a44-b980-ef05e81e5743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"background-color:#1a1a2e; padding:15px; border-radius:5px; margin-top:30px; margin-bottom:10px; border-left: 5px solid #00ff88;\">\n",
       "        <h3 style=\"color:white; margin:0; font-family:sans-serif;\">6. The Void Effect</h3>\n",
       "        <p style=\"color:#a5b1c2; margin:0; font-size:12px;\">Impact of Starting Distance on Completion % and EPA</p>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7c125 th {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_7c125_row0_col0, #T_7c125_row0_col1, #T_7c125_row0_col2, #T_7c125_row0_col3, #T_7c125_row0_col4, #T_7c125_row0_col5, #T_7c125_row1_col0, #T_7c125_row1_col1, #T_7c125_row1_col2, #T_7c125_row1_col3, #T_7c125_row1_col4, #T_7c125_row1_col5, #T_7c125_row2_col0, #T_7c125_row2_col1, #T_7c125_row2_col2, #T_7c125_row2_col3, #T_7c125_row2_col4, #T_7c125_row2_col5, #T_7c125_row3_col0, #T_7c125_row3_col1, #T_7c125_row3_col2, #T_7c125_row3_col3, #T_7c125_row3_col4, #T_7c125_row3_col5 {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7c125\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_7c125_level0_col0\" class=\"col_heading level0 col0\" >S_throw Band</th>\n",
       "      <th id=\"T_7c125_level0_col1\" class=\"col_heading level0 col1\" >Play Count</th>\n",
       "      <th id=\"T_7c125_level0_col2\" class=\"col_heading level0 col2\" >Completion %</th>\n",
       "      <th id=\"T_7c125_level0_col3\" class=\"col_heading level0 col3\" > from Tight</th>\n",
       "      <th id=\"T_7c125_level0_col4\" class=\"col_heading level0 col4\" >Avg EPA Allowed</th>\n",
       "      <th id=\"T_7c125_level0_col5\" class=\"col_heading level0 col5\" >Avg YAC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_7c125_row0_col0\" class=\"data row0 col0\" >Tight (0-2 yds)</td>\n",
       "      <td id=\"T_7c125_row0_col1\" class=\"data row0 col1\" >409</td>\n",
       "      <td id=\"T_7c125_row0_col2\" class=\"data row0 col2\" >48.7%</td>\n",
       "      <td id=\"T_7c125_row0_col3\" class=\"data row0 col3\" ></td>\n",
       "      <td id=\"T_7c125_row0_col4\" class=\"data row0 col4\" >0.16</td>\n",
       "      <td id=\"T_7c125_row0_col5\" class=\"data row0 col5\" >5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7c125_row1_col0\" class=\"data row1 col0\" >Medium (3-6 yds)</td>\n",
       "      <td id=\"T_7c125_row1_col1\" class=\"data row1 col1\" >1719</td>\n",
       "      <td id=\"T_7c125_row1_col2\" class=\"data row1 col2\" >73.4%</td>\n",
       "      <td id=\"T_7c125_row1_col3\" class=\"data row1 col3\" >+24.7pp</td>\n",
       "      <td id=\"T_7c125_row1_col4\" class=\"data row1 col4\" >0.29</td>\n",
       "      <td id=\"T_7c125_row1_col5\" class=\"data row1 col5\" >3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7c125_row2_col0\" class=\"data row2 col0\" >High Void (6-10 yds)</td>\n",
       "      <td id=\"T_7c125_row2_col1\" class=\"data row2 col1\" >1603</td>\n",
       "      <td id=\"T_7c125_row2_col2\" class=\"data row2 col2\" >74.7%</td>\n",
       "      <td id=\"T_7c125_row2_col3\" class=\"data row2 col3\" >+26.0pp</td>\n",
       "      <td id=\"T_7c125_row2_col4\" class=\"data row2 col4\" >0.40</td>\n",
       "      <td id=\"T_7c125_row2_col5\" class=\"data row2 col5\" >3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7c125_row3_col0\" class=\"data row3 col0\" >Deep (10+ yds)</td>\n",
       "      <td id=\"T_7c125_row3_col1\" class=\"data row3 col1\" >1460</td>\n",
       "      <td id=\"T_7c125_row3_col2\" class=\"data row3 col2\" >63.8%</td>\n",
       "      <td id=\"T_7c125_row3_col3\" class=\"data row3 col3\" >+15.1pp</td>\n",
       "      <td id=\"T_7c125_row3_col4\" class=\"data row3 col4\" >0.49</td>\n",
       "      <td id=\"T_7c125_row3_col5\" class=\"data row3 col5\" >4.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7a2dbcbf8410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- TABLE 6: VOID EFFECT SIZE ---\n",
    "df_void = gen.generate_void_effect_size()\n",
    "print_header(\"6. The Void Effect\", \"Impact of Starting Distance on Completion % and EPA\")\n",
    "\n",
    "st_void = (df_void.style\n",
    "           .format({'Avg EPA Allowed': '{:.2f}', 'Avg YAC': '{:.1f}'})\n",
    "           .set_properties(**{'text-align': 'center'})\n",
    "           .set_table_styles([{'selector': 'th', 'props': [('text-align', 'center')]}])\n",
    "           .hide(axis='index')\n",
    ")\n",
    "display(st_void)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1651acf-0fbc-4120-8ca7-c87165dfdd40",
   "metadata": {},
   "source": [
    "### Generate Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b953e34d-ebfd-4e48-9215-084aba7092e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_visual_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
